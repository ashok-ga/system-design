{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to System Design Hub","text":""},{"location":"#what-is-this","title":"\ud83d\ude80 What is This?","text":"<p>System Design Hub is a collaborative, open-source resource for learning, teaching, and documenting High-Level Design (HLD) and Low-Level Design (LLD) of software and hardware systems.</p> <p>Whether you are preparing for interviews, architecting a real-world solution, or seeking reference designs, you\u2019ll find clear explanations, diagrams, and design patterns here.</p>"},{"location":"#what-youll-find-here","title":"\ud83d\udcd6 What You'll Find Here","text":"<ul> <li>HLD Docs: Architectural overviews, technology choices, design decisions, major flows, and system context diagrams.</li> <li>LLD Docs: Detailed module specs, class diagrams, sequence diagrams, interface contracts, and logic flows.</li> <li>Diagrams &amp; Visuals: All key concepts are illustrated wherever possible.</li> <li>References: Further reading, links, standards, and best-practice guides.</li> <li>Contribution Guide: Friendly guidance for adding your own designs or improvements.</li> </ul>"},{"location":"#how-this-repository-is-organized","title":"\ud83d\uddc2\ufe0f How This Repository is Organized","text":"<ul> <li><code>hld/</code>: High-Level Design (system overviews, architectures, context)</li> <li><code>lld/</code>: Low-Level Design (detailed modules, APIs, data flows)</li> <li><code>assets/</code>: Shared images and diagrams</li> <li><code>references.md</code>: Papers, blog posts, books, and other resources</li> </ul> <p>Browse using the navigation sidebar or the links above!</p>"},{"location":"#how-to-contribute","title":"\ud83e\udd1d How to Contribute","text":"<p>We welcome your input! - Read the CONTRIBUTING.md for how to start. - Add your system designs, diagrams, or improvements. - Open issues for suggestions, bugs, or topic requests.</p> <p>Let\u2019s make system design knowledge accessible for all.</p>"},{"location":"#who-is-this-for","title":"\ud83d\udce3 Who is This For?","text":"<ul> <li>System architects</li> <li>Backend and full-stack developers</li> <li>Students and interview candidates</li> <li>Hardware and embedded designers</li> <li>Curious engineers</li> </ul> <p>If you want to understand, teach, or build real systems, this is for you!</p>"},{"location":"#star-the-repo-and-share-with-friends-if-you-find-it-useful","title":"\u2b50\ufe0f Star the repo and share with friends if you find it useful!","text":"<p>Happy Designing!</p>"},{"location":"references/","title":"References &amp; Further Reading","text":""},{"location":"references/#books","title":"Books","text":"<ul> <li>Designing Data-Intensive Applications \u2013 Martin Kleppmann</li> <li>System Design Interview \u2013 Alex Xu</li> </ul>"},{"location":"references/#blogs","title":"Blogs","text":"<ul> <li>Uber Engineering Blog</li> <li>Netflix Tech Blog</li> </ul>"},{"location":"references/#community","title":"Community","text":"<ul> <li>Awesome System Design</li> <li>System Design Primer</li> </ul>"},{"location":"references/#standards","title":"Standards","text":"<ul> <li>RFC 7231 (HTTP/1.1)</li> <li>REST API Design Guidelines (Microsoft)</li> </ul>"},{"location":"hld/architecture/","title":"System Architecture Example","text":""},{"location":"hld/architecture/#overview","title":"Overview","text":"<p>This document describes the architecture of a sample distributed system, including major components and their interactions.</p>"},{"location":"hld/architecture/#system-context-diagram","title":"System Context Diagram","text":""},{"location":"hld/architecture/#main-components","title":"Main Components","text":"<ul> <li>API Gateway: Handles all client requests and routing.</li> <li>Service Layer: Business logic is implemented in modular services.</li> <li>Database: Stores persistent data, supports replication and backup.</li> <li>Cache: Improves read performance and reduces DB load.</li> <li>Message Queue: Decouples services and enables async processing.</li> <li>Monitoring &amp; Logging: Provides observability.</li> </ul>"},{"location":"hld/architecture/#key-design-decisions","title":"Key Design Decisions","text":"<ul> <li>Technology stack: Python (FastAPI), PostgreSQL, Redis, RabbitMQ, Prometheus/Grafana</li> <li>Scalability: Each component can be scaled horizontally.</li> <li>Security: JWT-based authentication at API layer.</li> <li>Resilience: Retry policies, timeouts, health checks.</li> </ul>"},{"location":"hld/architecture/#sequence-diagram","title":"Sequence Diagram","text":"<p>Add relevant sequence diagrams for user flows here.</p> <p>Feel free to replace this sample with your own system\u2019s architecture!</p>"},{"location":"hld/de-shaw/","title":"DE Shaw HLD with Implementations","text":"<ul> <li>How to Approach</li> <li>Backtesting Platform (Distributed)</li> <li>CDC Data Platform (Operational DB - Lakehouse)</li> <li>Chat (1_1 + Typing + Read Receipts)</li> <li>Distributed Cache (Multi-Region)</li> <li>Distributed ID Generator (Snowflake-like)</li> <li>E-commerce Checkout</li> <li>Feature Flag Service (Local)</li> <li>HLD More</li> <li>In-Memory File System (Simplified)</li> <li>IoT Fleet Management</li> <li>Job Scheduler (cron + DAGs)</li> <li>Limit Order Book &amp; Matching Engine</li> <li>Meeting Room Scheduler</li> <li>Parking Lot</li> <li>Real-time Chat + Notifications</li> <li>Restaurant Delivery (Match Orders \u2194 Riders)</li> <li>Short-video Feed (TikTok-style)</li> <li>Splitwise-like Expense Splitter</li> <li>Threshold Alerting Engine</li> <li>Ticketing (High Contention)</li> <li>URL Shortener (Core)</li> <li>URL Shortener (Global)</li> <li>Vector Search Service</li> </ul>"},{"location":"hld/overview/","title":"High-Level Design (HLD) Overview","text":"<p>This section provides a bird\u2019s-eye view of system architectures, their major components, interactions, and key design decisions.</p>"},{"location":"hld/overview/#what-is-high-level-design","title":"What is High-Level Design?","text":"<p>High-Level Design (HLD) covers: - System architecture and main components - Data flow between subsystems - Technology stack choices - Major APIs and integrations - Non-functional considerations (scalability, fault-tolerance, security, etc.)</p>"},{"location":"hld/overview/#contents","title":"Contents","text":"<ul> <li>Architecture</li> <li>DE Shaw</li> </ul>"},{"location":"hld/overview/#when-to-use-hld","title":"When to use HLD?","text":"<ul> <li>During initial project scoping</li> <li>For design reviews and presentations</li> <li>When comparing architecture alternatives</li> </ul> <p>Start with <code>architecture.md</code> for an example system!</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Backtesting%20Platform%20%28Distributed%29/","title":"Backtesting Platform (Distributed)","text":""},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Backtesting%20Platform%20%28Distributed%29/#1-problem-statement-scope","title":"1. Problem Statement &amp; Scope","text":"<p>Design a distributed platform for quantitative analysts (\"quants\") to test their trading strategies against massive historical market data sets. The platform must be scalable to run hundreds of simulations concurrently and ensure that every backtest is perfectly reproducible.</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Backtesting%20Platform%20%28Distributed%29/#2-requirements","title":"2. Requirements","text":"<ul> <li>Functional Requirements:<ul> <li>Users can submit a backtest job with parameters: strategy code, date range, symbols, etc.</li> <li>The platform runs the strategy simulation over the specified historical data.</li> <li>The system must produce detailed results: performance metrics (Sharpe ratio, drawdown), a list of simulated trades, and logs.</li> <li>Reproducibility: A backtest run with the same code and data must produce the exact same result, bit for bit, every time.</li> </ul> </li> <li>Non-Functional Requirements:<ul> <li>Scalability: Handle 10+ TB of historical data and run hundreds of concurrent backtest jobs.</li> <li>Performance: A typical backtest of one strategy over one year of tick data should complete in a reasonable time (e.g., under 30 minutes).</li> <li>Isolation: Jobs from different users must be isolated in terms of resources and security.</li> <li>Cost-Effectiveness: Leverage cloud resources efficiently, potentially using cheaper spot instances for computation.</li> </ul> </li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Backtesting%20Platform%20%28Distributed%29/#3-capacity-estimation","title":"3. Capacity Estimation","text":"<ul> <li>Data Size: 10 TB of tick data. A single year for a single stock can be 50-100 GB.</li> <li>Concurrent Jobs: 200.</li> <li>Compute: If each job needs 4 CPU cores, we need <code>200 * 4 = 800</code> cores available.</li> <li>Data Access: A single job might need to read 100 GB of data. At 200 concurrent jobs, the peak read throughput from storage could be significant, necessitating a high-performance storage solution.</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Backtesting%20Platform%20%28Distributed%29/#4-high-level-architecture-diagram","title":"4. High-Level Architecture Diagram","text":"<p>Code snippet</p> <p>`graph TD     subgraph \"User Interface\"         A[Quant Analyst via UI/SDK] --&gt; B[Job API Service];     end</p> <pre><code>subgraph \"Control Plane\"\n    B --&gt; C[Job Scheduler];\n    C -- reads/writes --&gt; D[Metadata DB (Postgres)];\nend\n\nsubgraph \"Execution Plane (e.g., on Kubernetes)\"\n    C -- creates Pod --&gt; E{Backtest Runner Pod};\n    subgraph E\n        F[Containerized Strategy Code]\n    end\n    E --&gt; G[Data Access Layer];\nend\n\nsubgraph \"Data &amp; Storage\"\n    G --&gt; H[Object Storage (S3)];\n    H -- contains --&gt; I[Historical Data (Parquet)];\n    E -- writes logs/results --&gt; J[Results Store (S3)];\n    D -- stores --&gt; K[Job Metadata];\n    J -- triggers --&gt; L[Results Analysis Service]\nend`\n</code></pre>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Backtesting%20Platform%20%28Distributed%29/#5-data-schema-api-design","title":"5. Data Schema &amp; API Design","text":"<ul> <li>API:<ul> <li><code>POST /v1/jobs</code>: Submit a new backtest.<ul> <li>Body: <code>{\"strategy_image\": \"docker.io/repo/my_strategy:v1.2\", \"start_date\": \"...\", \"end_date\": \"...\", \"params\": {...}}</code></li> </ul> </li> <li><code>GET /v1/jobs/{job_id}</code>: Get the status and results of a job.</li> </ul> </li> <li>Data Models:<ul> <li>Metadata DB (Postgres):<ul> <li><code>jobs</code>: <code>job_id, user_id, status, docker_image_digest, submitted_at, results_path, ...</code></li> </ul> </li> <li>Storage (S3):<ul> <li>Historical Data: Stored in Parquet format, partitioned by date and symbol. <code>s3://market-data/ticks/symbol=AAPL/date=2025-08-14/data.parquet</code></li> <li>Results: Each job writes its output to a dedicated path. <code>s3://backtest-results/{job_id}/trades.csv</code>, <code>s3://backtest-results/{job_id}/metrics.json</code></li> </ul> </li> </ul> </li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Backtesting%20Platform%20%28Distributed%29/#6-detailed-component-breakdown","title":"6. Detailed Component Breakdown","text":"<ul> <li>Job API Service: The entry point for users. It authenticates the request, validates the parameters, and creates a new job record in the Metadata DB with a <code>PENDING</code> status.</li> <li>Job Scheduler: The core of the control plane. It can be a custom service or built on a workflow orchestrator like Argo Workflows or a simple Kubernetes Operator. It polls the database for <code>PENDING</code> jobs and submits them to the compute cluster for execution.</li> <li>Containerized Runner: This is the key to reproducibility. The user's strategy code and all its dependencies (e.g., specific versions of pandas, numpy) are packaged into a Docker image. The scheduler runs this specific, immutable image. This eliminates \"it works on my machine\" problems and guarantees an identical execution environment every time.</li> <li>Data Access Layer: A library or sidecar container within the runner pod responsible for efficiently fetching data. It understands the partitioning scheme of the data in S3. To optimize performance, it can implement a local cache on the worker node's SSD, so if another job needs the same data, it can be read from the fast local disk instead of S3. The scheduler can be made \"cache-aware\" to try and place jobs on nodes that already have the data.</li> <li>Object Storage (S3 + Parquet):<ul> <li>S3: Provides a cheap, scalable, and durable way to store terabytes of data.</li> <li>Parquet: A columnar storage format. This is extremely efficient for backtesting, as a strategy often only needs a few columns (e.g., <code>price</code>, <code>volume</code>) out of many. Columnar storage allows the runner to read only the data it needs, drastically reducing I/O.</li> </ul> </li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Backtesting%20Platform%20%28Distributed%29/#7-end-to-end-flow-submitting-a-backtest","title":"7. End-to-End Flow (Submitting a Backtest)","text":"<p>Code snippet</p> <p>`sequenceDiagram     participant Analyst     participant JobAPI     participant Scheduler     participant Kubernetes     participant RunnerPod     participant S3</p> <pre><code>Analyst-&gt;&gt;JobAPI: POST /jobs (strategy_image, params)\nJobAPI-&gt;&gt;JobAPI: Create job record in DB (status=PENDING)\nJobAPI--&gt;&gt;Analyst: 202 Accepted (job_id)\n\nloop Scheduler Loop\n    Scheduler-&gt;&gt;Scheduler: Poll DB for PENDING jobs.\n    Scheduler-&gt;&gt;Kubernetes: Create Pod from docker_image for job_id.\nend\n\nKubernetes-&gt;&gt;RunnerPod: Start container.\nRunnerPod-&gt;&gt;S3: Stream relevant Parquet data partitions.\nNote over RunnerPod: Execute strategy logic tick-by-tick...\nRunnerPod-&gt;&gt;S3: Write trades.csv and metrics.json to results path.\nRunnerPod-&gt;&gt;RunnerPod: Update job status to SUCCESS in DB.`\n</code></pre>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Backtesting%20Platform%20%28Distributed%29/#8-bottlenecks-fault-tolerance-and-trade-offs","title":"8. Bottlenecks, Fault Tolerance, and Trade-offs","text":"<ul> <li>Bottleneck: Data I/O: Reading large volumes of data from S3 can be the slowest part.<ul> <li>Mitigation: The columnar Parquet format is the primary mitigation. The local SSD cache on worker nodes is another. Using a high-performance query engine like DuckDB within the runner can also speed up data processing.</li> </ul> </li> <li>Fault Tolerance:<ul> <li>Runner Failure: The runner is a container. If it fails (e.g., out of memory), Kubernetes will automatically restart it based on the defined policy. Since the job is deterministic, it will restart from the beginning and produce the same result. The system can be enhanced with checkpointing for very long jobs.</li> </ul> </li> <li>Key Trade-offs:<ul> <li>Object Store (S3) vs. Distributed File System (HDFS): S3 is easier to manage, more cost-effective, and offers infinite scalability (decoupled compute and storage). HDFS can offer better performance if data locality is critical but comes with significant operational overhead. For this use case, the flexibility of S3 is usually preferred.</li> <li>Cost (Spot vs. On-Demand Instances): Backtesting jobs are often batch workloads that can tolerate interruption. Using cheaper EC2 Spot Instances can reduce compute costs by up to 90%. The trade-off is that a job might be preempted and need to be restarted, increasing its total wall-clock time.</li> </ul> </li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/CDC%20Data%20Platform%20%28Operational%20DB%20-%20Lakehouse%29/","title":"CDC Data Platform (Operational DB -&gt; Lakehouse)","text":""},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/CDC%20Data%20Platform%20%28Operational%20DB%20-%20Lakehouse%29/#1-problem-statement-scope","title":"1. Problem Statement &amp; Scope","text":"<p>Design a real-time Change Data Capture (CDC) platform to stream all changes from operational databases (e.g., Postgres, MySQL) into a central lakehouse (e.g., Iceberg, Delta Lake). The system must support high throughput, schema evolution, and strong delivery guarantees.</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/CDC%20Data%20Platform%20%28Operational%20DB%20-%20Lakehouse%29/#2-requirements","title":"2. Requirements","text":"<ul> <li>Functional Requirements:<ul> <li>Capture all inserts, updates, deletes from source DBs.</li> <li>Deliver changes to the lakehouse with at-least-once or exactly-once semantics.</li> <li>Handle schema evolution and DDL changes.</li> <li>Support multiple source DBs and tables.</li> </ul> </li> <li>Non-Functional Requirements:<ul> <li>Scalability: 100MB+/sec ingest, 1000+ tables.</li> <li>Reliability: No data loss, strong delivery guarantees.</li> <li>Extensibility: Add new sources/sinks easily.</li> </ul> </li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/CDC%20Data%20Platform%20%28Operational%20DB%20-%20Lakehouse%29/#3-capacity-estimation","title":"3. Capacity Estimation","text":"<ul> <li>Source DBs: 10, each 1TB, 100GB/day change rate.</li> <li>Change Rate: 10MB/sec per DB, 100MB/sec total.</li> <li>Lakehouse Storage: 10TB+.</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/CDC%20Data%20Platform%20%28Operational%20DB%20-%20Lakehouse%29/#4-high-level-architecture-diagram","title":"4. High-Level Architecture Diagram","text":"<p>Code snippet</p> <p><code>graph TD     A[Source DB (Postgres/MySQL)] --&gt; B[Debezium Connector];     B --&gt; C[Kafka (CDC Topics)];     C --&gt; D[Stream Processor (Flink/Spark)] ;     D --&gt; E[Lakehouse (Iceberg/Delta)];     D --&gt; F[Monitoring/Alerting];     C --&gt; G[Schema Registry];</code></p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/CDC%20Data%20Platform%20%28Operational%20DB%20-%20Lakehouse%29/#5-data-schema-api-design","title":"5. Data Schema &amp; API Design","text":"<ul> <li>CDC Event (Debezium): <code>{table, op (insert/update/delete), before, after, ts, schema_version}</code></li> <li>Kafka Topic: Partitioned by table, Avro/JSON encoding.</li> <li>Lakehouse Table: SCD2 (slowly changing dimension) or append-only, partitioned by date/table.</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/CDC%20Data%20Platform%20%28Operational%20DB%20-%20Lakehouse%29/#6-detailed-component-breakdown","title":"6. Detailed Component Breakdown","text":"<ul> <li>Debezium Connector: Reads DB logs, emits CDC events to Kafka. Handles schema changes.</li> <li>Kafka: Durable, scalable event bus. Buffers CDC events, supports replay.</li> <li>Stream Processor (Flink/Spark): Consumes CDC events, applies transformations, merges, and writes to lakehouse. Handles exactly-once via checkpointing.</li> <li>Lakehouse (Iceberg/Delta): Stores raw and processed data, supports ACID, schema evolution, and time travel.</li> <li>Schema Registry: Tracks schema versions for compatibility.</li> <li>Monitoring/Alerting: Tracks lag, failures, and data quality.</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/CDC%20Data%20Platform%20%28Operational%20DB%20-%20Lakehouse%29/#7-end-to-end-flow-cdc-ingest","title":"7. End-to-End Flow (CDC Ingest)","text":"<p>Code snippet</p> <p>`sequenceDiagram     participant SourceDB     participant Debezium     participant Kafka     participant Flink     participant Lakehouse</p> <pre><code>SourceDB-&gt;&gt;Debezium: Write to binlog/WAL\nDebezium-&gt;&gt;Kafka: Emit CDC event\nKafka-&gt;&gt;Flink: Stream event\nFlink-&gt;&gt;Lakehouse: Write/merge data\nFlink-&gt;&gt;Kafka: Commit offset (checkpoint)\nLakehouse--&gt;&gt;Flink: Ack\n</code></pre> <p>`</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/CDC%20Data%20Platform%20%28Operational%20DB%20-%20Lakehouse%29/#8-bottlenecks-fault-tolerance-and-trade-offs","title":"8. Bottlenecks, Fault Tolerance, and Trade-offs","text":"<ul> <li>Bottleneck: Stream Processor:<ul> <li>Scale out Flink/Spark for high ingest. Use partitioned topics.</li> </ul> </li> <li>Delivery Guarantees:<ul> <li>At-least-once is simpler, exactly-once requires checkpointing and idempotent writes.</li> </ul> </li> <li>Schema Evolution:<ul> <li>Use schema registry, support backward/forward compatibility.</li> </ul> </li> <li>Trade-offs:<ul> <li>Micro-batch merges are efficient but add latency. Row-by-row upserts are slower but more real-time.</li> </ul> </li> </ul> <p>This architecture is used by modern data platforms (e.g., Netflix, Uber) for real-time analytics and data lake ingestion.</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Chat%20%281_1%20%2B%20Typing%20%2B%20Read%20Receipts%29/","title":"Chat (1_1 + Typing + Read Receipts)","text":""},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Chat%20%281_1%20%2B%20Typing%20%2B%20Read%20Receipts%29/#problem-statement","title":"Problem Statement","text":"<p>Design a real-time chat system supporting 1:1 messaging, typing indicators, and read receipts. The system must provide low-latency delivery, message persistence, and seamless experience across devices.</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Chat%20%281_1%20%2B%20Typing%20%2B%20Read%20Receipts%29/#functional-requirements","title":"Functional Requirements","text":"<ul> <li>1:1 chat between users.</li> <li>Typing indicators (show when the other user is typing).</li> <li>Read receipts (show when a message is read).</li> <li>Message history and offline delivery.</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Chat%20%281_1%20%2B%20Typing%20%2B%20Read%20Receipts%29/#non-functional-requirements","title":"Non-Functional Requirements","text":"<ul> <li>Scalability: 10M+ users, 100k QPS.</li> <li>Low Latency: &lt;100ms message delivery.</li> <li>Reliability: No message loss.</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Chat%20%281_1%20%2B%20Typing%20%2B%20Read%20Receipts%29/#capacity-estimation","title":"Capacity Estimation","text":"<ul> <li>Users: 10M.</li> <li>Message Rate: 100k/sec peak.</li> <li>Storage: 1B messages/day, 1KB/message = 1TB/day.</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Chat%20%281_1%20%2B%20Typing%20%2B%20Read%20Receipts%29/#high-level-architecture-diagram","title":"High-Level Architecture Diagram","text":"<p>Code snippet</p> <p><code>graph TD     A[Client] --&gt; B[WebSocket Gateway];     B --&gt; C[Chat Service];     C --&gt; D[Message Store (NoSQL)];     C --&gt; E[Presence Service];     C --&gt; F[Typing Indicator Service];     C --&gt; G[Read Receipt Service];     D --&gt; H[Search Service];</code></p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Chat%20%281_1%20%2B%20Typing%20%2B%20Read%20Receipts%29/#data-schema-api-design","title":"Data Schema &amp; API Design","text":"<ul> <li>API:<ul> <li>WebSocket: <code>send_message</code>, <code>typing</code>, <code>read_receipt</code>, etc.</li> <li>REST: <code>GET /v1/messages</code>, <code>GET /v1/conversations</code>, etc.</li> </ul> </li> <li>Data Models:<ul> <li>Messages: <code>message_id, sender_id, receiver_id, content, ts, status</code></li> <li>Conversations: <code>conversation_id, participants, last_message_id, ...</code></li> <li>Presence: <code>user_id, status, last_seen</code></li> <li>Typing: <code>conversation_id, user_id, is_typing, ts</code></li> <li>ReadReceipt: <code>message_id, user_id, read_at</code></li> </ul> </li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Chat%20%281_1%20%2B%20Typing%20%2B%20Read%20Receipts%29/#detailed-component-breakdown","title":"Detailed Component Breakdown","text":"<ul> <li>WebSocket Gateway: Maintains persistent connections, authenticates users, routes messages.</li> <li>Chat Service: Core logic for message delivery, persistence, and fan-out.</li> <li>Message Store (NoSQL): Stores chat history, supports search and offline delivery.</li> <li>Presence Service: Tracks user status (online/offline).</li> <li>Typing Indicator Service: Publishes typing events to the other user.</li> <li>Read Receipt Service: Tracks and notifies when messages are read.</li> <li>Search Service: Indexes messages for fast retrieval.</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Chat%20%281_1%20%2B%20Typing%20%2B%20Read%20Receipts%29/#end-to-end-flow-message-send-read-receipt","title":"End-to-End Flow (Message Send &amp; Read Receipt)","text":"<p>Code snippet</p> <p>`sequenceDiagram     participant UserA     participant Gateway     participant ChatSvc     participant MessageStore     participant TypingSvc     participant ReadReceiptSvc     participant UserB</p> <pre><code>UserA-&gt;&gt;Gateway: send_message\nGateway-&gt;&gt;ChatSvc: Forward message\nChatSvc-&gt;&gt;MessageStore: Persist message\nChatSvc-&gt;&gt;UserB: Deliver message\nUserB-&gt;&gt;Gateway: typing\nGateway-&gt;&gt;TypingSvc: Publish typing\nTypingSvc-&gt;&gt;UserA: Show typing\nUserB-&gt;&gt;Gateway: read_receipt\nGateway-&gt;&gt;ReadReceiptSvc: Record read\nReadReceiptSvc-&gt;&gt;UserA: Notify read\n</code></pre> <p>`</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Chat%20%281_1%20%2B%20Typing%20%2B%20Read%20Receipts%29/#bottlenecks-fault-tolerance-and-trade-offs","title":"Bottlenecks, Fault Tolerance, and Trade-offs","text":"<ul> <li>Bottleneck: WebSocket Gateway:<ul> <li>Horizontally scalable, stateless. Use sticky sessions or consistent hashing.</li> </ul> </li> <li>Message Store:<ul> <li>NoSQL DB for high write throughput. Partition by conversation_id.</li> </ul> </li> <li>Typing/Read Receipts:<ul> <li>Pub-sub for real-time updates. Eventual consistency is acceptable.</li> </ul> </li> <li>Trade-offs:<ul> <li>Eventual consistency for presence/typing/read receipts. Strong consistency for message delivery.</li> </ul> </li> </ul> <p>This design is used by WhatsApp, Messenger, and other chat apps for real-time 1:1 messaging.</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Chat%20%281_1%20%2B%20Typing%20%2B%20Read%20Receipts%29/#chat-11-typing-read-receipts","title":"Chat (1:1 + Typing + Read Receipts)","text":""},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Chat%20%281_1%20%2B%20Typing%20%2B%20Read%20Receipts%29/#problem-statement_1","title":"Problem Statement","text":"<p>Design a simple chat service (no persistence required for LLD round). Support 1:1 chat, typing, and read receipts.</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Chat%20%281_1%20%2B%20Typing%20%2B%20Read%20Receipts%29/#functional-requirements_1","title":"Functional Requirements","text":"<ul> <li>Send message</li> <li>Delivery receipt</li> <li>Read receipt</li> <li>Typing indicator</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Chat%20%281_1%20%2B%20Typing%20%2B%20Read%20Receipts%29/#core-concepts","title":"Core Concepts","text":"<ul> <li><code>User</code>, <code>Conversation</code>, <code>Message</code>, <code>ChatService</code></li> <li><code>PresenceService</code> (in-memory), <code>Delivery</code> (Observer)</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Chat%20%281_1%20%2B%20Typing%20%2B%20Read%20Receipts%29/#high-level-design","title":"High-Level Design","text":"<ul> <li>Conversations:<ul> <li>Each conversation between two users</li> <li>Messages sent, delivered, read</li> </ul> </li> <li>Presence:<ul> <li>Track online/offline status</li> <li>Typing indicator via in-memory pub/sub</li> </ul> </li> <li>Delivery:<ul> <li>Observer pattern for delivery/read receipts</li> </ul> </li> <li>Edge Cases:<ul> <li>Ordering per conversation</li> <li>Idempotent resend</li> </ul> </li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Chat%20%281_1%20%2B%20Typing%20%2B%20Read%20Receipts%29/#step-by-step-solution","title":"Step-by-Step Solution","text":"<ol> <li>Define classes: User, Conversation, Message, ChatService</li> <li>PresenceService: in-memory status</li> <li>Delivery: observer for receipts</li> <li>API: send, typing, read</li> </ol>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Chat%20%281_1%20%2B%20Typing%20%2B%20Read%20Receipts%29/#edge-cases","title":"Edge Cases","text":"<ul> <li>Out-of-order delivery</li> <li>Duplicate messages</li> <li>User disconnects</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Distributed%20Cache%20%28Multi-Region%29/","title":"Distributed Cache (Multi-Region)","text":""},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Distributed%20Cache%20%28Multi-Region%29/#1-problem-statement-scope","title":"1. Problem Statement &amp; Scope","text":"<p>Design a highly available, low-latency, multi-region distributed cache to accelerate database reads for global applications. The system must support strong or eventual consistency, explicit invalidation, and seamless failover across regions.</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Distributed%20Cache%20%28Multi-Region%29/#2-requirements","title":"2. Requirements","text":"<ul> <li>Functional Requirements:<ul> <li>Store key-value data with TTL.</li> <li>Support read-through and write-through patterns.</li> <li>Explicit cache invalidation (by key, by pattern).</li> <li>Multi-region deployments with data locality.</li> </ul> </li> <li>Non-Functional Requirements:<ul> <li>Low Latency: &lt;10ms for local reads.</li> <li>High Availability: Survive region failures.</li> <li>Consistency: Configurable (eventual/strong).</li> <li>Scalability: 1M+ QPS, 10TB+ data.</li> </ul> </li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Distributed%20Cache%20%28Multi-Region%29/#3-capacity-estimation","title":"3. Capacity Estimation","text":"<ul> <li>Cache Size: 10TB total, 5 regions, 2TB/region.</li> <li>QPS: 1M/sec global, 200k/sec/region.</li> <li>Key Size: 64B avg, Value Size: 1KB avg.</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Distributed%20Cache%20%28Multi-Region%29/#4-high-level-architecture-diagram","title":"4. High-Level Architecture Diagram","text":"<p>Code snippet</p> <p><code>graph TD     A[App Server (Region 1)] --&gt; B[Cache Cluster (Region 1)];     A2[App Server (Region 2)] --&gt; C[Cache Cluster (Region 2)];     B &lt;--&gt; D[Global Invalidation Bus (Kafka)];     C &lt;--&gt; D;     B --&gt; E[Primary DB (Region 1)];     C --&gt; F[DB Replica (Region 2)];     D --&gt; B;     D --&gt; C;</code></p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Distributed%20Cache%20%28Multi-Region%29/#5-data-schema-api-design","title":"5. Data Schema &amp; API Design","text":"<ul> <li>API:<ul> <li><code>GET /cache/{key}</code></li> <li><code>SET /cache/{key}</code></li> <li><code>DEL /cache/{key}</code></li> <li><code>INVALIDATE /cache/{pattern}</code></li> </ul> </li> <li>Data Model:<ul> <li>Cache Entry: <code>{key, value, version, timestamp, ttl}</code></li> <li>Invalidation Event: <code>{key/pattern, version, region, timestamp}</code></li> </ul> </li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Distributed%20Cache%20%28Multi-Region%29/#6-detailed-component-breakdown","title":"6. Detailed Component Breakdown","text":"<ul> <li>Regional Cache Cluster: Redis/Memcached cluster per region. Handles local traffic for low latency.</li> <li>Global Invalidation Bus: Kafka or similar pub/sub for propagating invalidation events across regions.</li> <li>Primary DB + Replicas: Source of truth. Cache misses/read-throughs go here.</li> <li>Consistency Controller: Ensures strong/eventual consistency as configured.</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Distributed%20Cache%20%28Multi-Region%29/#7-end-to-end-flow-cache-readwriteinvalidate","title":"7. End-to-End Flow (Cache Read/Write/Invalidate)","text":"<p>Code snippet</p> <p>`sequenceDiagram     participant App     participant Cache     participant DB     participant InvalidationBus</p> <pre><code>App-&gt;&gt;Cache: GET key\nalt Hit\n    Cache--&gt;&gt;App: Return value\nelse Miss\n    Cache-&gt;&gt;DB: Fetch value\n    DB--&gt;&gt;Cache: Return value\n    Cache--&gt;&gt;App: Return value\n    Cache-&gt;&gt;Cache: SET key\nend\nApp-&gt;&gt;Cache: SET/DEL key\nCache-&gt;&gt;InvalidationBus: Publish invalidation\nInvalidationBus-&gt;&gt;Cache: Invalidate key in all regions\n</code></pre> <p>`</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Distributed%20Cache%20%28Multi-Region%29/#8-bottlenecks-fault-tolerance-and-trade-offs","title":"8. Bottlenecks, Fault Tolerance, and Trade-offs","text":"<ul> <li>Bottleneck: Invalidation Propagation:<ul> <li>Use async pub/sub for scale. Accept brief staleness for performance.</li> </ul> </li> <li>Consistency:<ul> <li>Eventual consistency is fast, but may serve stale data. Strong consistency is slower, requires cross-region coordination.</li> </ul> </li> <li>Availability:<ul> <li>Each region is independent. If one fails, others continue serving.</li> </ul> </li> <li>Trade-offs:<ul> <li>Eventual consistency is simpler and faster. Strong consistency is needed for critical data but adds latency.</li> </ul> </li> </ul> <p>This design is used by global-scale apps (e.g., Netflix, AWS ElastiCache Global Datastore) for low-latency, highly available caching.</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Distributed%20ID%20Generator%20%28Snowflake-like%29/","title":"Distributed ID Generator (Snowflake-like)","text":""},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Distributed%20ID%20Generator%20%28Snowflake-like%29/#1-problem-statement-scope","title":"1. Problem Statement &amp; Scope","text":"<p>Design a distributed ID generator that produces unique, sortable 64-bit IDs across multiple data centers and workers, similar to Twitter Snowflake. The system must handle clock skew, sequence rollover, and be highly available.</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Distributed%20ID%20Generator%20%28Snowflake-like%29/#2-requirements","title":"2. Requirements","text":"<ul> <li>Functional Requirements:<ul> <li>Generate unique 64-bit IDs: <code>timestamp | datacenter | worker | sequence</code>.</li> <li>IDs must be sortable by time.</li> <li>Handle clock skew, sequence rollover, and thread safety.</li> </ul> </li> <li>Non-Functional Requirements:<ul> <li>Low Latency: &lt;1ms per ID generation.</li> <li>High Availability: No duplicate IDs, even on failover.</li> <li>Scalability: 10k+ QPS per node, 1000+ nodes.</li> </ul> </li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Distributed%20ID%20Generator%20%28Snowflake-like%29/#3-capacity-estimation","title":"3. Capacity Estimation","text":"<ul> <li>Nodes: 1000 (workers across DCs).</li> <li>QPS: 10k/node = 10M QPS global.</li> <li>ID Space: 64 bits, 41b timestamp, 5b DC, 5b worker, 12b sequence.</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Distributed%20ID%20Generator%20%28Snowflake-like%29/#4-high-level-architecture-diagram","title":"4. High-Level Architecture Diagram","text":"<p>Code snippet</p> <p><code>graph TD     A[Client] --&gt; B[ID Generator Node];     B --&gt; C[Clock/Time Source];     B --&gt; D[Persistence (optional)];     B --&gt; E[Monitoring];     B --&gt; F[Cluster Coordination (optional)];</code></p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Distributed%20ID%20Generator%20%28Snowflake-like%29/#5-data-schema-api-design","title":"5. Data Schema &amp; API Design","text":"<ul> <li>API:<ul> <li><code>GET /v1/next_id</code>: Returns next unique ID.</li> </ul> </li> <li>ID Layout:<ul> <li>41 bits: timestamp (ms since custom epoch)</li> <li>5 bits: datacenter ID</li> <li>5 bits: worker ID</li> <li>12 bits: sequence (per ms)</li> </ul> </li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Distributed%20ID%20Generator%20%28Snowflake-like%29/#6-detailed-component-breakdown","title":"6. Detailed Component Breakdown","text":"<ul> <li>ID Generator Node: Implements the ID generation logic, maintains local state (last timestamp, sequence), and exposes API.</li> <li>Clock/Time Source: Uses system clock. If clock moves backward, node waits until safe.</li> <li>Persistence (optional): Stores last timestamp for crash recovery.</li> <li>Cluster Coordination (optional): Assigns unique worker/datacenter IDs, prevents collisions.</li> <li>Monitoring: Tracks QPS, errors, clock skew events.</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Distributed%20ID%20Generator%20%28Snowflake-like%29/#7-end-to-end-flow-id-generation","title":"7. End-to-End Flow (ID Generation)","text":"<p>Code snippet</p> <p>`sequenceDiagram     participant Client     participant IDGen     participant Clock     participant Persist</p> <pre><code>Client-&gt;&gt;IDGen: Request next_id\nIDGen-&gt;&gt;Clock: Get current timestamp\nalt Same ms as last\n    IDGen-&gt;&gt;IDGen: Increment sequence\n    alt Sequence overflow\n        IDGen-&gt;&gt;Clock: Wait for next ms\n    end\nelse Clock moved backward\n    IDGen-&gt;&gt;Clock: Wait until safe\nend\nIDGen-&gt;&gt;Persist: (Optional) Store last timestamp\nIDGen--&gt;&gt;Client: Return ID\n</code></pre> <p>`</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Distributed%20ID%20Generator%20%28Snowflake-like%29/#8-bottlenecks-fault-tolerance-and-trade-offs","title":"8. Bottlenecks, Fault Tolerance, and Trade-offs","text":"<ul> <li>Bottleneck: Sequence Overflow:<ul> <li>12 bits = 4096 IDs/ms/node. If exceeded, must wait for next ms.</li> </ul> </li> <li>Clock Skew:<ul> <li>If clock moves backward, node must wait. Use NTP and monotonic clocks.</li> </ul> </li> <li>Worker/DC ID Collisions:<ul> <li>Use static config or coordination service (e.g., Zookeeper) to assign unique IDs.</li> </ul> </li> <li>Persistence:<ul> <li>Optional for crash recovery. If not used, may generate duplicate IDs after crash.</li> </ul> </li> <li>Trade-offs:<ul> <li>Simpler design is stateless but risks duplicates on crash. Persistent design is safer but slower.</li> </ul> </li> </ul> <p>This design is used by Twitter, Instagram, and many distributed systems for unique, sortable ID generation.</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/E-commerce%20Checkout/","title":"E-commerce Checkout","text":""},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/E-commerce%20Checkout/#1-problem-statement-scope","title":"1. Problem Statement &amp; Scope","text":"<p>Design a robust, reliable, and scalable checkout system for an e-commerce platform. The system must handle order creation, inventory reservation, payment processing, and confirmation, ensuring no overselling or double-charging, even under failures or retries.</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/E-commerce%20Checkout/#2-requirements","title":"2. Requirements","text":"<ul> <li>Functional Requirements:<ul> <li>Create an order from a user's cart.</li> <li>Reserve inventory for each item.</li> <li>Process payment (credit card, wallet, etc.).</li> <li>Confirm order and notify user.</li> <li>Rollback all steps if any sub-step fails (distributed transaction).</li> </ul> </li> <li>Non-Functional Requirements:<ul> <li>Reliability: No double-charging, no overselling.</li> <li>Idempotency: Safe to retry any step.</li> <li>Scalability: Handle 10k+ checkouts/minute.</li> <li>Consistency: Strong consistency for inventory and payment.</li> <li>Observability: Trace each order's status.</li> </ul> </li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/E-commerce%20Checkout/#3-capacity-estimation","title":"3. Capacity Estimation","text":"<ul> <li>Peak Checkouts: 10k/minute (167/sec).</li> <li>Inventory Items/Order: Avg 3.</li> <li>Payment Failures: 1%.</li> <li>Storage: Orders table grows by 10M/year.</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/E-commerce%20Checkout/#4-high-level-architecture-diagram","title":"4. High-Level Architecture Diagram","text":"<p>Code snippet</p> <p><code>graph TD     A[User] --&gt; B[API Gateway];     B --&gt; C[Order Service (Saga Orchestrator)];     C --&gt; D[Inventory Service];     C --&gt; E[Payment Service];     C --&gt; F[Notification Service];     C --&gt; G[Outbox/Event Bus];     D --&gt; H[Inventory DB];     E --&gt; I[Payment Gateway];     F --&gt; J[Email/SMS];     G --&gt; K[Order Analytics];</code></p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/E-commerce%20Checkout/#5-data-schema-api-design","title":"5. Data Schema &amp; API Design","text":"<ul> <li>API:<ul> <li><code>POST /v1/checkout</code>: <code>{cart_id, payment_method, shipping_address, ...}</code></li> <li><code>GET /v1/orders/{order_id}</code>: Get order status.</li> </ul> </li> <li>Data Models:<ul> <li>Orders: <code>order_id, user_id, status, total, created_at, ...</code></li> <li>OrderItems: <code>order_id, item_id, qty, price</code></li> <li>Inventory: <code>item_id, available_qty</code></li> <li>Payments: <code>payment_id, order_id, status, amount, ...</code></li> <li>Idempotency Key: For safe retries.</li> </ul> </li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/E-commerce%20Checkout/#6-detailed-component-breakdown","title":"6. Detailed Component Breakdown","text":"<ul> <li>Order Service (Saga Orchestrator): Coordinates the distributed transaction. Starts the saga, tracks progress, and rolls back on failure.</li> <li>Inventory Service: Checks and reserves inventory. Supports compensation (release) on rollback.</li> <li>Payment Service: Processes payment. Supports refund on rollback.</li> <li>Notification Service: Sends order confirmation or failure notifications.</li> <li>Outbox/Event Bus: Ensures reliable event publishing for downstream consumers (analytics, fulfillment).</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/E-commerce%20Checkout/#7-end-to-end-flow-checkout-saga","title":"7. End-to-End Flow (Checkout Saga)","text":"<p>Code snippet</p> <p>`sequenceDiagram     participant User     participant API     participant OrderSvc     participant InventorySvc     participant PaymentSvc     participant NotificationSvc     participant Outbox</p> <pre><code>User-&gt;&gt;API: POST /checkout\nAPI-&gt;&gt;OrderSvc: Create order (PENDING)\nOrderSvc-&gt;&gt;InventorySvc: Reserve items\nInventorySvc--&gt;&gt;OrderSvc: Success/Fail\nOrderSvc-&gt;&gt;PaymentSvc: Charge payment\nPaymentSvc--&gt;&gt;OrderSvc: Success/Fail\nalt All succeed\n    OrderSvc-&gt;&gt;OrderSvc: Mark order CONFIRMED\n    OrderSvc-&gt;&gt;NotificationSvc: Send confirmation\n    OrderSvc-&gt;&gt;Outbox: Publish event\nelse Any fail\n    OrderSvc-&gt;&gt;InventorySvc: Release items (compensate)\n    OrderSvc-&gt;&gt;PaymentSvc: Refund (if needed)\n    OrderSvc-&gt;&gt;OrderSvc: Mark order FAILED\n    OrderSvc-&gt;&gt;NotificationSvc: Send failure\nend\n</code></pre> <p>`</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/E-commerce%20Checkout/#8-bottlenecks-fault-tolerance-and-trade-offs","title":"8. Bottlenecks, Fault Tolerance, and Trade-offs","text":"<ul> <li>Bottleneck: Inventory/Payment:<ul> <li>Both must be strongly consistent. Use row-level locks or atomic DB ops for inventory. Payment must be idempotent.</li> </ul> </li> <li>Saga Pattern:<ul> <li>Enables distributed rollback. Each step has a compensating action.</li> </ul> </li> <li>Outbox Pattern:<ul> <li>Ensures events are reliably published even if the service crashes after DB commit.</li> </ul> </li> <li>Trade-offs:<ul> <li>Strong consistency adds latency. Eventual consistency is possible for non-critical steps (e.g., notifications).</li> <li>Sagas are more complex than monolithic transactions but scale better and avoid distributed locks.</li> </ul> </li> </ul> <p>This design is used by leading e-commerce platforms to ensure reliability and scalability in the checkout process.</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Elevator%20Controller%20%28N%20Elevators%29/","title":"Elevator Controller (N Elevators)","text":""},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Elevator%20Controller%20%28N%20Elevators%29/#problem-statement","title":"Problem Statement","text":"<p>Schedule N elevators to minimize wait time. Handle hall/cab calls, group control, and safety.</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Elevator%20Controller%20%28N%20Elevators%29/#functional-requirements","title":"Functional Requirements","text":"<ul> <li>Handle hall &amp; cab calls</li> <li>States: IDLE, MOVING, DOOR_OPEN</li> <li>Group control, peak modes</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Elevator%20Controller%20%28N%20Elevators%29/#core-concepts","title":"Core Concepts","text":"<ul> <li><code>Elevator(id, currentFloor, direction, queue)</code> (State pattern)</li> <li><code>Scheduler.assign(call)</code> (Nearest Car / Look Algorithm)</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Elevator%20Controller%20%28N%20Elevators%29/#high-level-design","title":"High-Level Design","text":"<ul> <li>Elevator:<ul> <li>Each elevator tracks current floor, direction, and queue of stops</li> <li>State transitions: IDLE \u2194 MOVING \u2194 DOOR_OPEN</li> </ul> </li> <li>Scheduler:<ul> <li>Assigns calls to elevators (nearest car, look algorithm)</li> <li>Handles peak modes (morning/evening)</li> </ul> </li> <li>Safety:<ul> <li>Door interlocks, overload sensors</li> </ul> </li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Elevator%20Controller%20%28N%20Elevators%29/#step-by-step-solution","title":"Step-by-Step Solution","text":"<ol> <li>Elevator class: state, queue, move logic</li> <li>Scheduler: assign calls, optimize for wait time</li> <li>Safety: enforce interlocks</li> <li>Simulation: test with multiple elevators/calls</li> </ol>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Elevator%20Controller%20%28N%20Elevators%29/#edge-cases","title":"Edge Cases","text":"<ul> <li>Simultaneous calls</li> <li>Overload</li> <li>Emergency stop</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Feature%20Flag%20Service%20%28Local%29/","title":"Feature Flag Service (Local)","text":""},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Feature%20Flag%20Service%20%28Local%29/#1-problem-statement-scope","title":"1. Problem Statement &amp; Scope","text":"<p>Design a local feature flag service to enable/disable features for users or groups without redeploying code. The system must support targeting by user, group, or percentage, and provide low-latency flag evaluation in the app.</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Feature%20Flag%20Service%20%28Local%29/#2-requirements","title":"2. Requirements","text":"<ul> <li>Functional Requirements:<ul> <li>Create, update, delete feature flags.</li> <li>Target flags by user, group, or percentage rollout.</li> <li>Evaluate flags in the app (SDK or API).</li> </ul> </li> <li>Non-Functional Requirements:<ul> <li>Low Latency: &lt;5ms flag evaluation.</li> <li>Reliability: No flag loss, safe rollbacks.</li> <li>Consistency: Eventual consistency for local cache.</li> </ul> </li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Feature%20Flag%20Service%20%28Local%29/#3-capacity-estimation","title":"3. Capacity Estimation","text":"<ul> <li>Flags: 10k.</li> <li>Users: 1M.</li> <li>QPS: 10k/sec flag evaluations.</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Feature%20Flag%20Service%20%28Local%29/#4-high-level-architecture-diagram","title":"4. High-Level Architecture Diagram","text":"<p>Code snippet</p> <p><code>graph TD     A[Admin UI] --&gt; B[FeatureFlag Service];     B --&gt; C[Flag Store (DB)];     B --&gt; D[SDK Client];     D --&gt; E[App];     B --&gt; F[Cache];</code></p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Feature%20Flag%20Service%20%28Local%29/#5-data-schema-api-design","title":"5. Data Schema &amp; API Design","text":"<ul> <li>API:<ul> <li><code>POST /v1/flags</code>: Create flag.</li> <li><code>GET /v1/flags/{flag_id}</code>: Get flag.</li> <li><code>PUT /v1/flags/{flag_id}</code>: Update flag.</li> <li><code>DELETE /v1/flags/{flag_id}</code>: Delete flag.</li> </ul> </li> <li>Data Models:<ul> <li>Flag: <code>flag_id, name, enabled, targeting_rules, rollout_percentage, updated_at</code></li> <li>TargetingRule: <code>rule_id, flag_id, user_id/group_id, condition, value</code></li> </ul> </li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Feature%20Flag%20Service%20%28Local%29/#6-detailed-component-breakdown","title":"6. Detailed Component Breakdown","text":"<ul> <li>FeatureFlag Service: CRUD for flags, manages targeting rules, exposes API for SDKs.</li> <li>Flag Store (DB): Persistent storage for flags and rules.</li> <li>SDK Client: Fetches and caches flags locally, evaluates for user/group.</li> <li>Cache: In-memory cache for fast evaluation, supports TTL and refresh.</li> <li>Admin UI: For flag management and monitoring.</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Feature%20Flag%20Service%20%28Local%29/#7-end-to-end-flow-flag-evaluation","title":"7. End-to-End Flow (Flag Evaluation)","text":"<p>Code snippet</p> <p>`sequenceDiagram     participant Admin     participant FlagSvc     participant DB     participant SDK     participant App</p> <pre><code>Admin-&gt;&gt;FlagSvc: Create/Update flag\nFlagSvc-&gt;&gt;DB: Persist flag\nSDK-&gt;&gt;FlagSvc: Fetch flags\nFlagSvc--&gt;&gt;SDK: Return flags\nSDK-&gt;&gt;App: Evaluate flag for user\nApp--&gt;&gt;SDK: Use feature\n</code></pre> <p>`</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Feature%20Flag%20Service%20%28Local%29/#8-bottlenecks-fault-tolerance-and-trade-offs","title":"8. Bottlenecks, Fault Tolerance, and Trade-offs","text":"<ul> <li>Bottleneck: Cache Staleness:<ul> <li>Use TTL and background refresh. Accept brief staleness for performance.</li> </ul> </li> <li>Rollbacks:<ul> <li>Support instant disable/rollback. Use versioning for safe updates.</li> </ul> </li> <li>Trade-offs:<ul> <li>Local cache is fast but may be stale. Centralized API is slower but always fresh.</li> </ul> </li> </ul> <p>This design is used by LaunchDarkly, Unleash, and other feature flag platforms for local, low-latency flag evaluation.</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/HLD%20More/","title":"HLD More","text":""},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/HLD%20More/#high-level-design-expansions","title":"High-Level Design Expansions","text":"<p>This document contains detailed HLDs for selected systems from the main list. See the main README or other files for more.</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/How%20to%20Approach/","title":"How to Approach a System Design Interview","text":"<p>This guide provides a repeatable, industry-grade script for tackling any system design interview. Use it to structure your 45\u201360 minute discussion and demonstrate both depth and breadth in your thinking.</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/How%20to%20Approach/#1-lock-the-scope-clarify-requirements","title":"1. Lock the Scope (Clarify Requirements)","text":"<ul> <li>Ask clarifying questions: What is the core use case? What is out of scope? Who are the users? What are the must-have vs. nice-to-have features?</li> <li>Write down requirements: Separate functional (what the system does) and non-functional (scale, latency, availability, etc.).</li> <li>Confirm with interviewer: \"Just to confirm, for this session, we are focusing on X, Y, and Z, and not A or B. Is that correct?\"</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/How%20to%20Approach/#2-capacity-planning-back-of-the-envelope-estimation","title":"2. Capacity Planning (Back-of-the-Envelope Estimation)","text":"<ul> <li>Estimate scale: Users, QPS, data size, growth rate, peak vs. average load.</li> <li>Do quick math: E.g., \"If we expect 10M users, and each generates 100 requests/day, that's ~115 QPS.\"</li> <li>Document assumptions: State them clearly so you can adjust if the interviewer pushes back.</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/How%20to%20Approach/#3-api-data-model-design","title":"3. API &amp; Data Model Design","text":"<ul> <li>Define core APIs: REST/gRPC endpoints, request/response schemas, error handling, idempotency.</li> <li>Sketch data models: Tables, indexes, key fields, relationships. Consider partitioning and sharding keys.</li> <li>Discuss trade-offs: E.g., SQL vs. NoSQL, denormalization, consistency needs.</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/How%20to%20Approach/#4-high-level-architecture-boxes-arrows","title":"4. High-Level Architecture (Boxes &amp; Arrows)","text":"<ul> <li>Draw the big picture: Major components (clients, API gateway, services, DBs, caches, queues, etc.).</li> <li>Show data flow: How does a request travel through the system?</li> <li>Call out key patterns: E.g., load balancers, CDN, microservices, event-driven, etc.</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/How%20to%20Approach/#5-hot-path-deep-dive","title":"5. Hot Path Deep Dive","text":"<ul> <li>Pick a critical flow: E.g., \"Let's walk through a user posting a message.\"</li> <li>Sequence diagram: Step-by-step, from client to DB and back.</li> <li>Discuss latency, consistency, and failure points at each step.</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/How%20to%20Approach/#6-failure-consistency","title":"6. Failure &amp; Consistency","text":"<ul> <li>Identify failure modes: What if a DB is down? What if a message is lost?</li> <li>Mitigations: Retries, timeouts, circuit breakers, replication, backups.</li> <li>Consistency model: Strong, eventual, read-your-writes, etc. How do you guarantee it?</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/How%20to%20Approach/#7-bottlenecks-mitigations","title":"7. Bottlenecks &amp; Mitigations","text":"<ul> <li>Find the limits: What will break first as you scale? (DB, cache, network, etc.)</li> <li>Mitigation strategies: Caching, sharding, partitioning, async processing, rate limiting.</li> <li>Trade-offs: Simplicity vs. scalability, cost vs. performance.</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/How%20to%20Approach/#8-evolution-future-improvements","title":"8. Evolution (Future Improvements)","text":"<ul> <li>What would you do next? E.g., \"If we need to support 10x more users, I'd add X.\"</li> <li>Discuss extensibility: How would you add new features or support new use cases?</li> <li>Tech debt: What shortcuts did you take, and how would you address them later?</li> </ul> <p>Tips: - Always state your assumptions. - Use diagrams liberally (sequence, architecture, data model). - Prioritize clarity and structure over covering every possible detail. - If stuck, narrate your thought process and ask for hints.</p> <p>Example: See the main README or HLDs in this repo for detailed, step-by-step examples following this script.</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/In-Memory%20File%20System%20%28Simplified%29/","title":"In-Memory File System (Simplified)","text":""},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/In-Memory%20File%20System%20%28Simplified%29/#1-problem-statement-scope","title":"1. Problem Statement &amp; Scope","text":"<p>Design an in-memory file system supporting mkdir, ls, create, read, write, move, and delete operations. The system should mimic a real file system's API and structure, with fast, thread-safe operations and extensibility for future features (permissions, journaling).</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/In-Memory%20File%20System%20%28Simplified%29/#2-requirements","title":"2. Requirements","text":"<ul> <li>Functional Requirements:<ul> <li>Create/delete directories and files.</li> <li>List directory contents (ls).</li> <li>Read/write file data.</li> <li>Move/rename files and directories.</li> <li>Path resolution (absolute/relative).</li> </ul> </li> <li>Non-Functional Requirements:<ul> <li>Low Latency: All operations in-memory, &lt;1ms.</li> <li>Thread Safety: Support concurrent access.</li> <li>Extensibility: Permissions, journaling (future).</li> </ul> </li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/In-Memory%20File%20System%20%28Simplified%29/#3-capacity-estimation","title":"3. Capacity Estimation","text":"<ul> <li>Files/Dirs: 1M nodes.</li> <li>File Size: Up to 10MB/file.</li> <li>RAM: 10GB+ for large trees.</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/In-Memory%20File%20System%20%28Simplified%29/#4-high-level-architecture-diagram","title":"4. High-Level Architecture Diagram","text":"<p>Code snippet</p> <p><code>graph TD     A[User/API] --&gt; B[FileSystem];     B --&gt; C[Path Resolver];     B --&gt; D[Root Directory];     D --&gt; E[Directory/File Nodes];     B --&gt; F[Lock Manager];     B --&gt; G[Future: Journal/Log];</code></p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/In-Memory%20File%20System%20%28Simplified%29/#5-data-schema-api-design","title":"5. Data Schema &amp; API Design","text":"<ul> <li>API:<ul> <li><code>mkdir(path)</code></li> <li><code>ls(path)</code></li> <li><code>create(path, data)</code></li> <li><code>read(path)</code></li> <li><code>write(path, data)</code></li> <li><code>move(src, dst)</code></li> <li><code>delete(path)</code></li> </ul> </li> <li>Data Models:<ul> <li>Node: <code>name, type (file/dir), parent, metadata</code></li> <li>File: <code>name, data, size, created_at, updated_at</code></li> <li>Directory: <code>name, children (map)</code></li> </ul> </li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/In-Memory%20File%20System%20%28Simplified%29/#6-detailed-component-breakdown","title":"6. Detailed Component Breakdown","text":"<ul> <li>FileSystem: Main API, manages root, path resolution, and delegates to nodes.</li> <li>Path Resolver: Parses and resolves absolute/relative paths.</li> <li>Directory Node: Holds children (files/dirs) in a map for fast lookup.</li> <li>File Node: Stores file data and metadata.</li> <li>Lock Manager: Ensures thread safety for concurrent ops.</li> <li>Journal/Log (Future): For durability and crash recovery.</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/In-Memory%20File%20System%20%28Simplified%29/#7-end-to-end-flow-file-create-move","title":"7. End-to-End Flow (File Create &amp; Move)","text":"<p>Code snippet</p> <p>`sequenceDiagram     participant User     participant FS     participant PathRes     participant Dir     participant File</p> <pre><code>User-&gt;&gt;FS: create(/foo/bar.txt, data)\nFS-&gt;&gt;PathRes: Resolve /foo\nPathRes--&gt;&gt;FS: Directory node\nFS-&gt;&gt;Dir: Add File node\nDir-&gt;&gt;File: Store data\nUser-&gt;&gt;FS: move(/foo/bar.txt, /baz/bar.txt)\nFS-&gt;&gt;PathRes: Resolve /foo/bar.txt, /baz\nPathRes--&gt;&gt;FS: Source file, dest dir\nFS-&gt;&gt;Dir: Remove from /foo, add to /baz\n</code></pre> <p>`</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/In-Memory%20File%20System%20%28Simplified%29/#8-bottlenecks-fault-tolerance-and-trade-offs","title":"8. Bottlenecks, Fault Tolerance, and Trade-offs","text":"<ul> <li>Bottleneck: Path Resolution:<ul> <li>Use trie/map for fast lookup. Cache resolved paths.</li> </ul> </li> <li>Thread Safety:<ul> <li>Use fine-grained locks or lock-free structures for concurrency.</li> </ul> </li> <li>Durability:<ul> <li>In-memory only; add journaling for persistence.</li> </ul> </li> <li>Trade-offs:<ul> <li>In-memory is fast but volatile. Adding journaling increases durability but adds latency.</li> </ul> </li> </ul> <p>This design is used in interview questions and as a basis for real file system implementations.</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/IoT%20Fleet%20Management/","title":"IoT Fleet Management","text":""},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/IoT%20Fleet%20Management/#1-problem-statement-scope","title":"1. Problem Statement &amp; Scope","text":"<p>Design a cloud-native platform to manage a large fleet of IoT devices. The system must support secure onboarding, high-volume telemetry ingestion, remote control, device shadowing, and over-the-air (OTA) firmware updates, with high reliability and scalability.</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/IoT%20Fleet%20Management/#2-requirements","title":"2. Requirements","text":"<ul> <li>Functional Requirements:<ul> <li>Secure device onboarding and authentication.</li> <li>Ingest and store high-frequency telemetry from devices.</li> <li>Remote command/control (e.g., reboot, config update).</li> <li>Device shadow (digital twin) for state sync.</li> <li>OTA firmware updates to devices.</li> </ul> </li> <li>Non-Functional Requirements:<ul> <li>Scalability: 1M+ devices, 10k+ messages/sec.</li> <li>Reliability: No data loss, strong delivery guarantees.</li> <li>Security: TLS, mutual auth, role-based access.</li> </ul> </li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/IoT%20Fleet%20Management/#3-capacity-estimation","title":"3. Capacity Estimation","text":"<ul> <li>Devices: 1M.</li> <li>Telemetry Rate: 1 msg/sec/device = 1M/sec.</li> <li>Storage: 1KB/msg, 1TB/day.</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/IoT%20Fleet%20Management/#4-high-level-architecture-diagram","title":"4. High-Level Architecture Diagram","text":"<p>Code snippet</p> <p><code>graph TD     A[Device] --&gt; B[MQTT Broker Cluster];     B --&gt; C[Device Registry];     B --&gt; D[Device Shadow Service];     B --&gt; E[Rules Engine];     E --&gt; F[Time-series DB];     E --&gt; G[OTA Update Service];     D --&gt; H[Shadow DB];     G --&gt; I[Firmware Store];     C --&gt; J[User Portal/API];</code></p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/IoT%20Fleet%20Management/#5-data-schema-api-design","title":"5. Data Schema &amp; API Design","text":"<ul> <li>API:<ul> <li><code>POST /v1/devices</code>: Register device.</li> <li><code>POST /v1/devices/{id}/command</code>: Send command.</li> <li><code>POST /v1/ota/deployments</code>: Deploy firmware.</li> </ul> </li> <li>Data Models:<ul> <li>Device Registry: <code>device_id, cert, status, metadata</code></li> <li>Device Shadow: <code>device_id, reported, desired, last_sync</code></li> <li>Telemetry: <code>device_id, ts, payload</code></li> <li>OTA Deployment: <code>deployment_id, firmware_url, status</code></li> </ul> </li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/IoT%20Fleet%20Management/#6-detailed-component-breakdown","title":"6. Detailed Component Breakdown","text":"<ul> <li>MQTT Broker Cluster: Handles device connections, message routing, and QoS.</li> <li>Device Registry: Stores device identities, certificates, and metadata.</li> <li>Device Shadow Service: Maintains digital twin for each device, syncs state.</li> <li>Rules Engine: Processes telemetry, triggers alerts/actions.</li> <li>Time-series DB: Stores telemetry for analytics and monitoring.</li> <li>OTA Update Service: Manages firmware deployments, tracks status.</li> <li>Firmware Store: Stores firmware binaries.</li> <li>User Portal/API: For device management and monitoring.</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/IoT%20Fleet%20Management/#7-end-to-end-flow-telemetry-ingest-ota-update","title":"7. End-to-End Flow (Telemetry Ingest &amp; OTA Update)","text":"<p>Code snippet</p> <p>`sequenceDiagram     participant Device     participant MQTT     participant Registry     participant Shadow     participant Rules     participant TSDB     participant OTA     participant Firmware</p> <pre><code>Device-&gt;&gt;MQTT: Connect &amp; authenticate\nMQTT-&gt;&gt;Registry: Validate device\nDevice-&gt;&gt;MQTT: Publish telemetry\nMQTT-&gt;&gt;Rules: Route message\nRules-&gt;&gt;TSDB: Store telemetry\nUser-&gt;&gt;OTA: Deploy firmware\nOTA-&gt;&gt;Firmware: Fetch binary\nOTA-&gt;&gt;MQTT: Notify device\nDevice-&gt;&gt;MQTT: Download &amp; update\nMQTT-&gt;&gt;OTA: Report status\n</code></pre> <p>`</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/IoT%20Fleet%20Management/#8-bottlenecks-fault-tolerance-and-trade-offs","title":"8. Bottlenecks, Fault Tolerance, and Trade-offs","text":"<ul> <li>Bottleneck: MQTT Broker:<ul> <li>Use clustering, horizontal scaling. Partition by device_id.</li> </ul> </li> <li>Telemetry Storage:<ul> <li>Time-series DB optimized for high ingest.</li> </ul> </li> <li>OTA Updates:<ul> <li>Staged rollout, retries for reliability.</li> </ul> </li> <li>Trade-offs:<ul> <li>MQTT is efficient for IoT, but HTTP is simpler for some use cases.</li> <li>Device shadow enables offline sync but adds complexity.</li> </ul> </li> </ul> <p>This design is used by AWS IoT, Azure IoT Hub, and other large-scale IoT platforms.</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Job%20Scheduler%20%28cron%20%2B%20DAGs%29/","title":"Job Scheduler (cron + DAGs)","text":""},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Job%20Scheduler%20%28cron%20%2B%20DAGs%29/#1-problem-statement-scope","title":"1. Problem Statement &amp; Scope","text":"<p>Design a reliable, scalable job scheduling and orchestration platform for cron and DAG-based workflows (like Airflow/Prefect). The system must support complex dependencies, retries, monitoring, and high availability.</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Job%20Scheduler%20%28cron%20%2B%20DAGs%29/#2-requirements","title":"2. Requirements","text":"<ul> <li>Functional Requirements:<ul> <li>Define workflows as DAGs (Directed Acyclic Graphs) of tasks.</li> <li>Trigger jobs on schedule (cron) or manually.</li> <li>Manage dependencies, retries, backoff, and failure handling.</li> <li>UI/API for workflow management and monitoring.</li> </ul> </li> <li>Non-Functional Requirements:<ul> <li>Scalability: 10k+ concurrent jobs.</li> <li>Reliability: No job loss, strong guarantees.</li> <li>Extensibility: Support custom operators, plugins.</li> </ul> </li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Job%20Scheduler%20%28cron%20%2B%20DAGs%29/#3-capacity-estimation","title":"3. Capacity Estimation","text":"<ul> <li>Workflows: 10k active DAGs.</li> <li>Tasks/DAG: Avg 10.</li> <li>Job Rate: 1k/sec peak.</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Job%20Scheduler%20%28cron%20%2B%20DAGs%29/#4-high-level-architecture-diagram","title":"4. High-Level Architecture Diagram","text":"<p>Code snippet</p> <p><code>graph TD     A[User/UI/API] --&gt; B[Scheduler];     B --&gt; C[Metadata DB (Postgres)];     B --&gt; D[Message Queue (RabbitMQ/Kafka)];     D --&gt; E[Worker Fleet];     E --&gt; F[Object Storage];     B --&gt; G[Monitoring/Alerting];     B --&gt; H[UI/API];</code></p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Job%20Scheduler%20%28cron%20%2B%20DAGs%29/#5-data-schema-api-design","title":"5. Data Schema &amp; API Design","text":"<ul> <li>API:<ul> <li><code>POST /v1/dags</code>: Create DAG.</li> <li><code>POST /v1/dags/{dag_id}/runs</code>: Trigger run.</li> <li><code>GET /v1/dags/{dag_id}/runs/{run_id}</code>: Get status.</li> </ul> </li> <li>Data Models:<ul> <li>DAG: <code>dag_id, definition, schedule, owner, ...</code></li> <li>DagRun: <code>run_id, dag_id, status, start_time, end_time</code></li> <li>TaskInstance: <code>task_id, run_id, status, retries, logs</code></li> </ul> </li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Job%20Scheduler%20%28cron%20%2B%20DAGs%29/#6-detailed-component-breakdown","title":"6. Detailed Component Breakdown","text":"<ul> <li>Scheduler: Core orchestrator. Evaluates schedules, triggers DAG runs, manages state.</li> <li>Metadata DB: Stores DAG definitions, run history, task state.</li> <li>Message Queue: Decouples scheduling from execution. Ensures reliable delivery of tasks to workers.</li> <li>Worker Fleet: Stateless workers execute tasks. Can autoscale.</li> <li>Object Storage: Stores logs, artifacts, and large outputs.</li> <li>Monitoring/Alerting: Tracks job status, failures, and metrics.</li> <li>UI/API: For workflow management and monitoring.</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Job%20Scheduler%20%28cron%20%2B%20DAGs%29/#7-end-to-end-flow-dag-run","title":"7. End-to-End Flow (DAG Run)","text":"<p>Code snippet</p> <p>`sequenceDiagram     participant User     participant Scheduler     participant DB     participant Queue     participant Worker     participant Storage</p> <pre><code>User-&gt;&gt;Scheduler: Trigger DAG run\nScheduler-&gt;&gt;DB: Create DagRun\nScheduler-&gt;&gt;Queue: Enqueue tasks\nQueue-&gt;&gt;Worker: Fetch task\nWorker-&gt;&gt;Storage: Write logs/artifacts\nWorker-&gt;&gt;DB: Update task status\nScheduler-&gt;&gt;User: Update status\n</code></pre> <p>`</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Job%20Scheduler%20%28cron%20%2B%20DAGs%29/#8-bottlenecks-fault-tolerance-and-trade-offs","title":"8. Bottlenecks, Fault Tolerance, and Trade-offs","text":"<ul> <li>Bottleneck: Scheduler/Queue:<ul> <li>Use active-passive HA for scheduler. Partition queues for scale.</li> </ul> </li> <li>Workers:<ul> <li>Stateless, autoscale. Use heartbeat/zombie detector to handle failures.</li> </ul> </li> <li>Reliability:<ul> <li>All state in DB. Queue is durable. Failed tasks retried with backoff.</li> </ul> </li> <li>Trade-offs:<ul> <li>Strong guarantees add complexity. Eventual consistency is possible for non-critical metrics.</li> </ul> </li> </ul> <p>This design is used by modern workflow engines (Airflow, Prefect, Argo) for reliable, scalable job orchestration.</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Limit%20Order%20Book%20%26%20Matching%20Engine/","title":"Limit Order Book &amp; Matching Engine","text":""},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Limit%20Order%20Book%20%26%20Matching%20Engine/#1-problem-statement-scope","title":"1. Problem Statement &amp; Scope","text":"<p>Design a high-performance, deterministic matching engine for a single financial instrument (e.g., AAPL stock). The engine must process new/cancel orders, match trades using strict price-time priority, and ensure durability and fault tolerance. The system should support high throughput (100k+ orders/sec), low latency (&lt;1ms per match), and be the source of truth for all trades.</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Limit%20Order%20Book%20%26%20Matching%20Engine/#2-requirements","title":"2. Requirements","text":"<ul> <li>Functional Requirements:<ul> <li>Accept new limit and market orders (buy/sell).</li> <li>Accept order cancellation and modification requests.</li> <li>Match orders using price-time priority (FIFO within price level).</li> <li>Support order types: Limit, Market, IOC (Immediate or Cancel), FOK (Fill or Kill).</li> <li>Generate trade execution reports and order status updates.</li> </ul> </li> <li>Non-Functional Requirements:<ul> <li>Performance: &lt;1ms matching latency, 100k+ orders/sec.</li> <li>Determinism: Same input sequence always produces same output.</li> <li>Durability: No loss of orders/trades on crash (WAL, snapshots).</li> <li>Availability: 99.99% uptime, fast failover.</li> <li>Auditability: Full replay and audit trail.</li> </ul> </li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Limit%20Order%20Book%20%26%20Matching%20Engine/#3-capacity-estimation","title":"3. Capacity Estimation","text":"<ul> <li>Order Rate: 100k orders/sec peak.</li> <li>Order Book Depth: 10k price levels, 1M open orders.</li> <li>Trade Rate: 10k trades/sec.</li> <li>Storage: Each order ~200 bytes, 1M open orders = 200MB in-memory. WAL: 100k orders/sec * 200B = 20MB/sec, ~1.7TB/day.</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Limit%20Order%20Book%20%26%20Matching%20Engine/#4-high-level-architecture-diagram","title":"4. High-Level Architecture Diagram","text":"<p>Code snippet</p> <p><code>graph TD     A[Client Gateway] --&gt; B[Sequencer];     B --&gt; C[Matching Engine (Single-threaded)];     C --&gt; D[WAL Persister];     C --&gt; E[Snapshotter];     C --&gt; F[Event Bus];     F --&gt; G[Downstream Consumers (Analytics, Risk, UI)];     D --&gt; H[Durable Storage];     E --&gt; H;     B --&gt; I[Order Book State];     C --&gt; I;     I --&gt; C;</code></p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Limit%20Order%20Book%20%26%20Matching%20Engine/#5-data-schema-api-design","title":"5. Data Schema &amp; API Design","text":"<ul> <li>API:<ul> <li><code>NewOrderSingle</code>: <code>{order_id, side, price, qty, type, tif, user_id}</code></li> <li><code>OrderCancelRequest</code>: <code>{order_id, user_id}</code></li> <li><code>OrderReplaceRequest</code>: <code>{order_id, new_qty, new_price}</code></li> <li><code>ExecutionReport</code>: <code>{order_id, status, fill_qty, fill_price, ...}</code></li> </ul> </li> <li>Order Book:<ul> <li>Two price-ordered trees (bids, asks), each price \u2192 FIFO queue of orders.</li> <li>In-memory, with periodic snapshots and WAL for durability.</li> </ul> </li> <li>WAL (Write-Ahead Log):<ul> <li>Append-only log of all order events, persisted before ack.</li> </ul> </li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Limit%20Order%20Book%20%26%20Matching%20Engine/#6-detailed-component-breakdown","title":"6. Detailed Component Breakdown","text":"<ul> <li>Client Gateway: Authenticates clients, validates messages, and forwards to sequencer.</li> <li>Sequencer: Assigns a global sequence number to all incoming messages, ensuring total ordering and preventing race conditions.</li> <li>Matching Engine: Single-threaded for determinism. Maintains in-memory order book, processes events in order, matches trades, and generates execution reports.</li> <li>WAL Persister: Writes every event to disk before ack. Enables crash recovery and replay.</li> <li>Snapshotter: Periodically saves full in-memory state for fast recovery.</li> <li>Event Bus: Publishes all events (order, trade, cancel) to downstream consumers (risk, analytics, UI).</li> <li>Durable Storage: Stores WAL and snapshots for audit and replay.</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Limit%20Order%20Book%20%26%20Matching%20Engine/#7-end-to-end-flow-order-submission-matching","title":"7. End-to-End Flow (Order Submission &amp; Matching)","text":"<p>Code snippet</p> <p>`sequenceDiagram     participant Trader     participant Gateway     participant Sequencer     participant MatchingEngine     participant WAL     participant Snapshotter     participant EventBus</p> <pre><code>Trader-&gt;&gt;Gateway: NewOrderSingle\nGateway-&gt;&gt;Sequencer: Validate &amp; Forward\nSequencer-&gt;&gt;MatchingEngine: Assign sequence, forward event\nMatchingEngine-&gt;&gt;WAL: Write event to WAL\nWAL--&gt;&gt;MatchingEngine: Ack\nMatchingEngine-&gt;&gt;MatchingEngine: Update order book, match trades\nMatchingEngine-&gt;&gt;Snapshotter: Periodic snapshot\nMatchingEngine-&gt;&gt;EventBus: Publish ExecutionReport\nEventBus--&gt;&gt;Trader: ExecutionReport\n</code></pre> <p>`</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Limit%20Order%20Book%20%26%20Matching%20Engine/#8-bottlenecks-fault-tolerance-and-trade-offs","title":"8. Bottlenecks, Fault Tolerance, and Trade-offs","text":"<ul> <li>Bottleneck: Single-threaded Engine:<ul> <li>Ensures determinism but limits vertical scaling. Mitigation: Partition by instrument (one engine per symbol).</li> </ul> </li> <li>Durability:<ul> <li>WAL + periodic snapshots. On crash, replay WAL from last snapshot.</li> </ul> </li> <li>Availability:<ul> <li>Hot standby replica can replay WAL in real-time for fast failover.</li> </ul> </li> <li>Trade-offs:<ul> <li>Single-threaded = simple, deterministic, but not horizontally scalable for one instrument.</li> <li>WAL = strong durability, but adds write latency.</li> <li>Partitioning by instrument enables horizontal scale.</li> </ul> </li> </ul> <p>This design is used in real-world exchanges (e.g., NASDAQ, NYSE) for its determinism, auditability, and performance.</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Meeting%20Room%20Scheduler/","title":"Meeting Room Scheduler","text":""},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Meeting%20Room%20Scheduler/#problem-statement","title":"Problem Statement","text":"<p>Book rooms avoiding conflicts, search by capacity/resources, support recurring bookings.</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Meeting%20Room%20Scheduler/#functional-requirements","title":"Functional Requirements","text":"<ul> <li>Create rooms</li> <li>Book/cancel</li> <li>Search free slots</li> <li>Recurring bookings</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Meeting%20Room%20Scheduler/#core-concepts","title":"Core Concepts","text":"<ul> <li><code>Room(id, capacity, features)</code></li> <li><code>Booking(roomId, interval, user)</code></li> <li><code>IntervalTree</code> or ordered map per room for conflict checks</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Meeting%20Room%20Scheduler/#high-level-design","title":"High-Level Design","text":"<ul> <li>Room Management:<ul> <li>Add/edit/delete rooms</li> <li>Track features (A/V, whiteboard, etc.)</li> </ul> </li> <li>Booking:<ul> <li>Book/cancel with conflict checks</li> <li>Recurring bookings: expand to individual slots</li> </ul> </li> <li>Search:<ul> <li>Find free rooms by time/capacity/features</li> </ul> </li> <li>Edge Cases:<ul> <li>Buffer times between meetings</li> <li>Recurrence expansion</li> </ul> </li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Meeting%20Room%20Scheduler/#step-by-step-solution","title":"Step-by-Step Solution","text":"<ol> <li>Define classes: Room, Booking, IntervalTree</li> <li>Booking logic: conflict checks</li> <li>Search: by time/capacity/features</li> <li>API: create room, book, search</li> </ol>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Meeting%20Room%20Scheduler/#edge-cases","title":"Edge Cases","text":"<ul> <li>Overlapping bookings</li> <li>Recurring conflicts</li> <li>Room feature changes</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Metrics%20Store%20%28Append-only%20%2B%20Compaction%29/","title":"Metrics Store (Append-only + Compaction)","text":""},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Metrics%20Store%20%28Append-only%20%2B%20Compaction%29/#problem-statement","title":"Problem Statement","text":"<p>In-process time-series store with write/fetch and periodic compaction.</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Metrics%20Store%20%28Append-only%20%2B%20Compaction%29/#functional-requirements","title":"Functional Requirements","text":"<ul> <li><code>put(metric, ts, value)</code>, <code>rangeQuery(metric, from, to)</code></li> <li>Compaction to downsample older data (1s\u21921m)</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Metrics%20Store%20%28Append-only%20%2B%20Compaction%29/#core-concepts","title":"Core Concepts","text":"<ul> <li><code>Segment</code> (append log), <code>Index</code>, <code>Compactor</code></li> <li>Memory-mapped segments (optional)</li> <li>Read path merges raw+compact</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Metrics%20Store%20%28Append-only%20%2B%20Compaction%29/#high-level-design","title":"High-Level Design","text":"<ul> <li>Segments:<ul> <li>Append-only log per metric</li> <li>Index for fast lookup</li> </ul> </li> <li>Compaction:<ul> <li>Downsample old data (e.g., 1s \u2192 1m)</li> <li>Merge segments</li> </ul> </li> <li>API:<ul> <li>put, rangeQuery</li> </ul> </li> <li>Edge Cases:<ul> <li>Out-of-order writes</li> <li>Query across raw+compact</li> </ul> </li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Metrics%20Store%20%28Append-only%20%2B%20Compaction%29/#step-by-step-solution","title":"Step-by-Step Solution","text":"<ol> <li>Define Segment, Index, Compactor classes</li> <li>Write path: append to segment</li> <li>Compaction: periodic downsampling</li> <li>API: put, rangeQuery</li> </ol>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Metrics%20Store%20%28Append-only%20%2B%20Compaction%29/#edge-cases","title":"Edge Cases","text":"<ul> <li>Data loss on crash (in-memory)</li> <li>Compaction lag</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Parking%20Lot/","title":"Parking Lot","text":""},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Parking%20Lot/#problem-statement","title":"Problem Statement","text":"<p>Design a multi-floor parking lot with spot types and pricing. Support park/unpark, nearest spot allocation, and billing.</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Parking%20Lot/#functional-requirements","title":"Functional Requirements","text":"<ul> <li>Park/unpark vehicle</li> <li>Find nearest suitable spot</li> <li>Track tickets and fees</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Parking%20Lot/#core-concepts","title":"Core Concepts","text":"<ul> <li><code>ParkingLot</code>, <code>Floor</code>, <code>Spot(type, id, isFree)</code>, <code>Vehicle(type)</code></li> <li><code>Allocator</code> (nearest/cheapest)</li> <li><code>Ticket</code>, <code>Billing</code></li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Parking%20Lot/#high-level-design","title":"High-Level Design","text":"<ul> <li>Structure:<ul> <li>ParkingLot contains Floors</li> <li>Each Floor has Spots (by type: regular, compact, handicapped, etc.)</li> </ul> </li> <li>Allocation:<ul> <li>On park, find nearest free spot of required type</li> <li>On unpark, calculate fee, free spot</li> </ul> </li> <li>Ticketing:<ul> <li>Issue ticket on entry, track time</li> <li>On exit, calculate fee (duration \u00d7 rate)</li> </ul> </li> <li>Edge Cases:<ul> <li>Overflow (lot full)</li> <li>Reservations</li> <li>Lost tickets</li> </ul> </li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Parking%20Lot/#step-by-step-solution","title":"Step-by-Step Solution","text":"<ol> <li>Define classes: ParkingLot, Floor, Spot, Vehicle, Ticket</li> <li>Allocator: nearest/cheapest strategy</li> <li>Billing: calculate fees</li> <li>API: park, unpark, status endpoints</li> </ol>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Parking%20Lot/#edge-cases","title":"Edge Cases","text":"<ul> <li>Multiple vehicle types</li> <li>Lost ticket handling</li> <li>Dynamic pricing</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Real-time%20Chat%20%2B%20Notifications/","title":"Real-time Chat + Notifications","text":""},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Real-time%20Chat%20%2B%20Notifications/#1-problem-statement-scope","title":"1. Problem Statement &amp; Scope","text":"<p>Design a scalable, low-latency chat platform supporting 1:1 and group messaging, user presence, read receipts, and push notifications. The system must handle millions of concurrent users, guarantee message delivery, and provide a seamless experience across devices.</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Real-time%20Chat%20%2B%20Notifications/#2-requirements","title":"2. Requirements","text":"<ul> <li>Functional Requirements:<ul> <li>Send/receive messages in real time (1:1, group).</li> <li>Show user presence (online/offline/away).</li> <li>Read receipts and typing indicators.</li> <li>Persist chat history and support search.</li> <li>Push notifications for offline users.</li> </ul> </li> <li>Non-Functional Requirements:<ul> <li>Scalability: Millions of concurrent users, 100k+ QPS.</li> <li>Low Latency: &lt;100ms end-to-end delivery.</li> <li>Reliability: No message loss, at-least-once delivery.</li> <li>Security: End-to-end encryption (optional).</li> </ul> </li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Real-time%20Chat%20%2B%20Notifications/#3-capacity-estimation","title":"3. Capacity Estimation","text":"<ul> <li>Concurrent Users: 10M.</li> <li>Message Rate: 100k/sec peak.</li> <li>Storage: 1B messages/day, 1KB/message = 1TB/day.</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Real-time%20Chat%20%2B%20Notifications/#4-high-level-architecture-diagram","title":"4. High-Level Architecture Diagram","text":"<p>Code snippet</p> <p><code>graph TD     A[Client] --&gt; B[WebSocket Gateway];     B --&gt; C[Presence Service];     B --&gt; D[Chat Service];     D --&gt; E[Message Store (Cassandra/ScyllaDB)];     D --&gt; F[Kafka/Event Bus];     F --&gt; G[Fan-out Service];     G --&gt; H[Push Notification Service];     D --&gt; I[Search Service];     C --&gt; J[Presence DB/Cache];</code></p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Real-time%20Chat%20%2B%20Notifications/#5-data-schema-api-design","title":"5. Data Schema &amp; API Design","text":"<ul> <li>API:<ul> <li>WebSocket: <code>send_message</code>, <code>receive_message</code>, <code>presence_update</code>, <code>typing</code>, etc.</li> <li>REST: <code>GET /v1/conversations</code>, <code>GET /v1/messages</code>, etc.</li> </ul> </li> <li>Data Models:<ul> <li>Messages: <code>message_id, conversation_id, sender_id, content, timestamp, status</code></li> <li>Conversations: <code>conversation_id, participants, last_message_id, ...</code></li> <li>Presence: <code>user_id, status, last_seen</code></li> </ul> </li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Real-time%20Chat%20%2B%20Notifications/#6-detailed-component-breakdown","title":"6. Detailed Component Breakdown","text":"<ul> <li>WebSocket Gateway: Maintains persistent connections, authenticates users, routes messages to Chat Service.</li> <li>Presence Service: Tracks user status, updates presence in real time, and notifies interested clients.</li> <li>Chat Service: Core logic for message delivery, persistence, and fan-out. Handles message ordering and deduplication.</li> <li>Message Store: Scalable NoSQL DB (Cassandra/ScyllaDB) for chat history.</li> <li>Kafka/Event Bus: Decouples message ingestion from fan-out and notification.</li> <li>Fan-out Service: Delivers messages to all recipients (online/offline), triggers push notifications as needed.</li> <li>Push Notification Service: Integrates with APNs/FCM for mobile push.</li> <li>Search Service: Indexes messages for fast retrieval.</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Real-time%20Chat%20%2B%20Notifications/#7-end-to-end-flow-message-send-delivery","title":"7. End-to-End Flow (Message Send &amp; Delivery)","text":"<p>Code snippet</p> <p>`sequenceDiagram     participant UserA     participant Gateway     participant ChatSvc     participant MessageStore     participant Kafka     participant Fanout     participant PushSvc     participant UserB</p> <pre><code>UserA-&gt;&gt;Gateway: send_message\nGateway-&gt;&gt;ChatSvc: Forward message\nChatSvc-&gt;&gt;MessageStore: Persist message\nChatSvc-&gt;&gt;Kafka: Publish event\nKafka-&gt;&gt;Fanout: Consume event\nFanout-&gt;&gt;UserB: Deliver message (if online)\nFanout-&gt;&gt;PushSvc: Push notification (if offline)\nPushSvc--&gt;&gt;UserB: Mobile push\nUserB-&gt;&gt;Gateway: ack/read_receipt\nGateway-&gt;&gt;ChatSvc: Update status\n</code></pre> <p>`</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Real-time%20Chat%20%2B%20Notifications/#8-bottlenecks-fault-tolerance-and-trade-offs","title":"8. Bottlenecks, Fault Tolerance, and Trade-offs","text":"<ul> <li>Bottleneck: WebSocket Gateway:<ul> <li>Horizontally scalable, stateless. Use sticky sessions or consistent hashing.</li> </ul> </li> <li>Message Store:<ul> <li>NoSQL DB for high write throughput. Partition by conversation_id.</li> </ul> </li> <li>Fan-out:<ul> <li>Kafka decouples ingestion from delivery. Enables retries and at-least-once delivery.</li> </ul> </li> <li>Trade-offs:<ul> <li>Eventual consistency for presence/read receipts. Strong consistency for message delivery.</li> <li>Push notifications may be delayed by mobile OS.</li> </ul> </li> </ul> <p>This architecture is used by leading chat apps (WhatsApp, Slack, Messenger) for reliability and scale.</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Restaurant%20Delivery%20%28Match%20Orders%20%E2%86%94%20Riders%29/","title":"Restaurant Delivery (Match Orders \u2194 Riders)","text":""},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Restaurant%20Delivery%20%28Match%20Orders%20%E2%86%94%20Riders%29/#1-problem-statement-scope","title":"1. Problem Statement &amp; Scope","text":"<p>Design a scalable backend for a food delivery platform that matches orders with available riders in real time. The system must optimize for fast delivery, live tracking, dynamic pricing, and high availability during peak demand.</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Restaurant%20Delivery%20%28Match%20Orders%20%E2%86%94%20Riders%29/#2-requirements","title":"2. Requirements","text":"<ul> <li>Functional Requirements:<ul> <li>Ingest real-time rider locations (GPS updates).</li> <li>Accept and manage new food orders.</li> <li>Assign best rider to each order using a scoring model (ETA, proximity, load).</li> <li>Live tracking for users and restaurants.</li> <li>Dynamic/surge pricing based on demand.</li> </ul> </li> <li>Non-Functional Requirements:<ul> <li>Scalability: 1M+ orders/day, 100k+ concurrent riders.</li> <li>Low Latency: &lt;1s for assignment.</li> <li>Reliability: No lost orders, high uptime.</li> </ul> </li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Restaurant%20Delivery%20%28Match%20Orders%20%E2%86%94%20Riders%29/#3-capacity-estimation","title":"3. Capacity Estimation","text":"<ul> <li>Orders: 1M/day (~12/sec avg, 1k/sec peak).</li> <li>Riders: 100k active.</li> <li>Location Updates: 1/min/rider = 100k/min.</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Restaurant%20Delivery%20%28Match%20Orders%20%E2%86%94%20Riders%29/#4-high-level-architecture-diagram","title":"4. High-Level Architecture Diagram","text":"<p>Code snippet</p> <p><code>graph TD     A[User App] --&gt; B[Order Service];     C[Rider App] --&gt; D[Telemetry Service];     B --&gt; E[Assignment/Dispatch Service];     D --&gt; E;     E --&gt; F[Geo-Index Service];     E --&gt; G[ETA Service];     E --&gt; H[Pricing Service];     E --&gt; I[Orders DB];     E --&gt; J[Rider Profile DB];     E --&gt; K[Notification Service];</code></p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Restaurant%20Delivery%20%28Match%20Orders%20%E2%86%94%20Riders%29/#5-data-schema-api-design","title":"5. Data Schema &amp; API Design","text":"<ul> <li>API:<ul> <li><code>POST /v1/orders</code>: Place new order.</li> <li><code>POST /v1/riders/location</code>: Update rider location.</li> <li><code>GET /v1/orders/{order_id}/status</code>: Track order.</li> </ul> </li> <li>Data Models:<ul> <li>Orders: <code>order_id, user_id, restaurant_id, status, assigned_rider, ...</code></li> <li>Riders: <code>rider_id, location, status, capacity, ...</code></li> <li>Geo-Index: Spatial index for fast proximity search.</li> </ul> </li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Restaurant%20Delivery%20%28Match%20Orders%20%E2%86%94%20Riders%29/#6-detailed-component-breakdown","title":"6. Detailed Component Breakdown","text":"<ul> <li>Order Service: Handles order creation, status, and user notifications.</li> <li>Telemetry Service: Ingests and stores real-time rider locations.</li> <li>Assignment/Dispatch Service: Runs matching algorithm, assigns riders, and triggers notifications.</li> <li>Geo-Index Service: Maintains spatial index for fast nearest-neighbor queries.</li> <li>ETA Service: Estimates delivery times using traffic, distance, and rider load.</li> <li>Pricing Service: Calculates dynamic pricing based on demand/supply.</li> <li>Notification Service: Sends updates to users, riders, and restaurants.</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Restaurant%20Delivery%20%28Match%20Orders%20%E2%86%94%20Riders%29/#7-end-to-end-flow-order-assignment","title":"7. End-to-End Flow (Order Assignment)","text":"<p>Code snippet</p> <p>`sequenceDiagram     participant User     participant OrderSvc     participant Telemetry     participant Dispatch     participant GeoIndex     participant Rider     participant Notification</p> <pre><code>User-&gt;&gt;OrderSvc: Place order\nOrderSvc-&gt;&gt;Dispatch: New order event\nTelemetry-&gt;&gt;Dispatch: Rider location update\nDispatch-&gt;&gt;GeoIndex: Find nearby riders\nGeoIndex--&gt;&gt;Dispatch: Candidate riders\nDispatch-&gt;&gt;Dispatch: Score &amp; select best rider\nDispatch-&gt;&gt;Rider: Assign order\nDispatch-&gt;&gt;OrderSvc: Update order status\nDispatch-&gt;&gt;Notification: Notify user/restaurant\nRider-&gt;&gt;OrderSvc: Accept/reject\nOrderSvc-&gt;&gt;Dispatch: Update status\n</code></pre> <p>`</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Restaurant%20Delivery%20%28Match%20Orders%20%E2%86%94%20Riders%29/#8-bottlenecks-fault-tolerance-and-trade-offs","title":"8. Bottlenecks, Fault Tolerance, and Trade-offs","text":"<ul> <li>Bottleneck: Geo-Index:<ul> <li>Use in-memory spatial index (e.g., Redis Geo) for fast lookups. Partition by city/region.</li> </ul> </li> <li>Assignment Algorithm:<ul> <li>Greedy (fast) vs. batch (optimal). Greedy is simple, batch is more efficient but adds latency.</li> </ul> </li> <li>Reliability:<ul> <li>All state changes are persisted. Use retries and dead-letter queues for failed assignments.</li> </ul> </li> <li>Trade-offs:<ul> <li>Greedy matching is fast but may be suboptimal. Batch matching is optimal but slower.</li> <li>Real-time tracking is resource-intensive but improves user experience.</li> </ul> </li> </ul> <p>This design is used by Uber Eats, DoorDash, and Swiggy for real-time order-to-rider matching.</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Short-video%20Feed%20%28TikTok-style%29/","title":"Short-video Feed (TikTok-style)","text":""},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Short-video%20Feed%20%28TikTok-style%29/#1-problem-statement-scope","title":"1. Problem Statement &amp; Scope","text":"<p>Design a scalable backend for a short-video mobile app (like TikTok) that delivers a personalized, infinite feed. The system must support low-latency video delivery, real-time user interactions, and high availability for millions of users.</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Short-video%20Feed%20%28TikTok-style%29/#2-requirements","title":"2. Requirements","text":"<ul> <li>Functional Requirements:<ul> <li>Generate personalized, infinite video feeds per user.</li> <li>Support user interactions: like, comment, share, follow.</li> <li>Real-time event ingestion for engagement signals.</li> <li>Low TTFB (time to first byte) for video playback.</li> </ul> </li> <li>Non-Functional Requirements:<ul> <li>Scalability: 100M+ users, 1M QPS.</li> <li>Low Latency: &lt;100ms feed load, &lt;1s video start.</li> <li>Availability: 99.99% uptime.</li> </ul> </li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Short-video%20Feed%20%28TikTok-style%29/#3-capacity-estimation","title":"3. Capacity Estimation","text":"<ul> <li>Users: 100M.</li> <li>Videos: 1B+.</li> <li>Feed Requests: 1M QPS.</li> <li>Video Size: 5MB avg, 5PB total.</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Short-video%20Feed%20%28TikTok-style%29/#4-high-level-architecture-diagram","title":"4. High-Level Architecture Diagram","text":"<p>Code snippet</p> <p><code>graph TD     A[User] --&gt; B[API Gateway];     B --&gt; C[Feed Orchestrator];     C --&gt; D[Candidate Generators];     C --&gt; E[Feature Service];     C --&gt; F[Ranker];     F --&gt; G[Filtering/Composition];     G --&gt; H[CDN (Video Assets)];     C --&gt; I[Event Ingestion];     I --&gt; J[Analytics/ML];</code></p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Short-video%20Feed%20%28TikTok-style%29/#5-data-schema-api-design","title":"5. Data Schema &amp; API Design","text":"<ul> <li>API:<ul> <li><code>GET /v1/feed</code>: Fetch personalized feed.</li> <li><code>POST /v1/videos/{video_id}/like</code>: Like a video.</li> <li><code>POST /v1/videos/{video_id}/comment</code>: Comment on a video.</li> </ul> </li> <li>Data Models:<ul> <li>Videos: <code>video_id, uploader_id, url, metadata, features</code></li> <li>Users: <code>user_id, profile, preferences, history</code></li> <li>Events: <code>event_id, user_id, video_id, type, ts</code></li> </ul> </li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Short-video%20Feed%20%28TikTok-style%29/#6-detailed-component-breakdown","title":"6. Detailed Component Breakdown","text":"<ul> <li>Feed Orchestrator: Entry point for feed requests. Coordinates candidate generation, ranking, and filtering.</li> <li>Candidate Generators: Generate a pool of potential videos (fresh, trending, followed, etc.).</li> <li>Feature Service: Computes user/video features for ranking (e.g., embeddings, engagement).</li> <li>Ranker: ML model ranks candidates for personalization.</li> <li>Filtering/Composition: Removes seen/ineligible videos, composes final feed.</li> <li>CDN: Delivers video assets with low latency.</li> <li>Event Ingestion: Streams user interactions to analytics/ML for feedback loop.</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Short-video%20Feed%20%28TikTok-style%29/#7-end-to-end-flow-feed-generation","title":"7. End-to-End Flow (Feed Generation)","text":"<p>Code snippet</p> <p>`sequenceDiagram     participant User     participant APIGW     participant Orchestrator     participant CandidateGen     participant FeatureSvc     participant Ranker     participant Filter     participant CDN</p> <pre><code>User-&gt;&gt;APIGW: GET /feed\nAPIGW-&gt;&gt;Orchestrator: Forward request\nOrchestrator-&gt;&gt;CandidateGen: Get candidates\nCandidateGen--&gt;&gt;Orchestrator: Candidate list\nOrchestrator-&gt;&gt;FeatureSvc: Get features\nFeatureSvc--&gt;&gt;Orchestrator: Features\nOrchestrator-&gt;&gt;Ranker: Rank candidates\nRanker--&gt;&gt;Orchestrator: Ranked list\nOrchestrator-&gt;&gt;Filter: Filter/compose\nFilter--&gt;&gt;Orchestrator: Final feed\nOrchestrator-&gt;&gt;APIGW: Return feed\nAPIGW-&gt;&gt;User: Show feed\nUser-&gt;&gt;CDN: Fetch video asset\nCDN--&gt;&gt;User: Stream video\n</code></pre> <p>`</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Short-video%20Feed%20%28TikTok-style%29/#8-bottlenecks-fault-tolerance-and-trade-offs","title":"8. Bottlenecks, Fault Tolerance, and Trade-offs","text":"<ul> <li>Bottleneck: Feed Generation:<ul> <li>Use precomputed feeds for cold start, real-time for active users.</li> </ul> </li> <li>Video Delivery:<ul> <li>CDN ensures low-latency, global delivery. Multi-CDN for redundancy.</li> </ul> </li> <li>Personalization:<ul> <li>ML ranking is compute-intensive. Use feature stores and caching.</li> </ul> </li> <li>Trade-offs:<ul> <li>Real-time feeds are more personalized but costlier. Precomputed feeds are faster but less fresh.</li> </ul> </li> </ul> <p>This design is used by TikTok, Instagram Reels, and YouTube Shorts for scalable, personalized video feeds.</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Splitwise-like%20Expense%20Splitter/","title":"Splitwise-like Expense Splitter","text":""},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Splitwise-like%20Expense%20Splitter/#1-problem-statement-scope","title":"1. Problem Statement &amp; Scope","text":"<p>Design a system to track group expenses, balances, and simplify debt among users (like Splitwise). The system must support adding expenses, calculating balances, and settling up efficiently, even for large groups and multiple currencies.</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Splitwise-like%20Expense%20Splitter/#2-requirements","title":"2. Requirements","text":"<ul> <li>Functional Requirements:<ul> <li>Add expenses (equal, unequal, percent split).</li> <li>Track balances per user and group.</li> <li>Simplify debts (minimize cash flow between users).</li> <li>Settle up (record payments, mark debts as settled).</li> </ul> </li> <li>Non-Functional Requirements:<ul> <li>Scalability: 1M+ users, 100k+ groups.</li> <li>Accuracy: Handle rounding, currency conversion.</li> <li>Reliability: No lost transactions.</li> </ul> </li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Splitwise-like%20Expense%20Splitter/#3-capacity-estimation","title":"3. Capacity Estimation","text":"<ul> <li>Users: 1M.</li> <li>Groups: 100k.</li> <li>Expenses: 10M/month.</li> <li>Storage: Each expense ~200B, 10M = 2GB/month.</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Splitwise-like%20Expense%20Splitter/#4-high-level-architecture-diagram","title":"4. High-Level Architecture Diagram","text":"<p>Code snippet</p> <p><code>graph TD     A[User App] --&gt; B[API Gateway];     B --&gt; C[Expense Service];     C --&gt; D[Balance Service];     C --&gt; E[Settlement Engine];     C --&gt; F[Notification Service];     D --&gt; G[User DB];     C --&gt; H[Group DB];     C --&gt; I[Expense DB];     E --&gt; J[Payment Service];</code></p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Splitwise-like%20Expense%20Splitter/#5-data-schema-api-design","title":"5. Data Schema &amp; API Design","text":"<ul> <li>API:<ul> <li><code>POST /v1/groups/{group_id}/expenses</code>: Add expense.</li> <li><code>GET /v1/groups/{group_id}/balances</code>: Get balances.</li> <li><code>POST /v1/groups/{group_id}/settle</code>: Settle up.</li> </ul> </li> <li>Data Models:<ul> <li>User: <code>user_id, name, email, ...</code></li> <li>Group: <code>group_id, name, members, ...</code></li> <li>Expense: <code>expense_id, group_id, paid_by, amount, split, currency, ts</code></li> <li>BalanceSheet: <code>group_id, user_id, balance</code></li> <li>Settlement: <code>settlement_id, group_id, from_user, to_user, amount, ts</code></li> </ul> </li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Splitwise-like%20Expense%20Splitter/#6-detailed-component-breakdown","title":"6. Detailed Component Breakdown","text":"<ul> <li>Expense Service: Handles adding expenses, validates splits, updates balances.</li> <li>Balance Service: Calculates and stores per-user balances for each group.</li> <li>Settlement Engine: Runs min-cash-flow algorithm to minimize number of payments needed to settle all debts.</li> <li>Notification Service: Notifies users of new expenses, settlements, or reminders.</li> <li>User/Group/Expense DBs: Store all persistent data.</li> <li>Payment Service: (Optional) Integrates with payment gateways for real money settlements.</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Splitwise-like%20Expense%20Splitter/#7-end-to-end-flow-add-expense-settle-up","title":"7. End-to-End Flow (Add Expense &amp; Settle Up)","text":"<p>Code snippet</p> <p>`sequenceDiagram     participant User     participant API     participant ExpenseSvc     participant BalanceSvc     participant SettlementEng     participant Notification</p> <pre><code>User-&gt;&gt;API: Add expense\nAPI-&gt;&gt;ExpenseSvc: Validate &amp; record\nExpenseSvc-&gt;&gt;BalanceSvc: Update balances\nExpenseSvc-&gt;&gt;Notification: Notify group\nUser-&gt;&gt;API: Settle up\nAPI-&gt;&gt;SettlementEng: Run min-cash-flow\nSettlementEng-&gt;&gt;BalanceSvc: Update balances\nSettlementEng-&gt;&gt;Notification: Notify users\n</code></pre> <p>`</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Splitwise-like%20Expense%20Splitter/#8-bottlenecks-fault-tolerance-and-trade-offs","title":"8. Bottlenecks, Fault Tolerance, and Trade-offs","text":"<ul> <li>Bottleneck: Settlement Algorithm:<ul> <li>Min-cash-flow is O(N^2) for large groups. Use heuristics or batch settlements for scale.</li> </ul> </li> <li>Accuracy:<ul> <li>Rounding and currency conversion can cause small errors. Use high-precision types and audit logs.</li> </ul> </li> <li>Reliability:<ul> <li>All transactions are persisted. Use idempotency keys for safe retries.</li> </ul> </li> <li>Trade-offs:<ul> <li>Simpler algorithms are faster but may not minimize payments. More complex ones are optimal but slower.</li> </ul> </li> </ul> <p>This design is used by Splitwise and similar apps for group expense management and debt simplification.</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Threshold%20Alerting%20Engine/","title":"Threshold Alerting Engine","text":""},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Threshold%20Alerting%20Engine/#problem-statement","title":"Problem Statement","text":"<p>On time-series inputs, trigger alerts (e.g., CPU&gt;90% for 5m) with debounce.</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Threshold%20Alerting%20Engine/#functional-requirements","title":"Functional Requirements","text":"<ul> <li>Rules per metric</li> <li>Conditions across time windows</li> <li>Suppress duplicates (debounce)</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Threshold%20Alerting%20Engine/#core-concepts","title":"Core Concepts","text":"<ul> <li><code>Rule(metric, predicate, window, duration)</code></li> <li><code>Evaluator</code> keeps rolling window</li> <li><code>Notifier</code> (email/webhook)</li> <li>Debounce logic</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Threshold%20Alerting%20Engine/#high-level-design","title":"High-Level Design","text":"<ul> <li>Rules:<ul> <li>Each rule: metric, predicate, window, duration</li> </ul> </li> <li>Evaluator:<ul> <li>Maintains rolling window of values</li> <li>Triggers alert if condition met for duration</li> </ul> </li> <li>Notifier:<ul> <li>Sends alert (email/webhook)</li> <li>Debounce to suppress duplicates</li> </ul> </li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Threshold%20Alerting%20Engine/#step-by-step-solution","title":"Step-by-Step Solution","text":"<ol> <li>Define Rule, Evaluator, Notifier classes</li> <li>Evaluator: rolling window logic</li> <li>Debounce: suppress duplicate alerts</li> <li>API: add rule, ingest metric, get alerts</li> </ol>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Threshold%20Alerting%20Engine/#edge-cases","title":"Edge Cases","text":"<ul> <li>Flapping metrics</li> <li>Overlapping rules</li> <li>Missed alerts on downtime</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Ticketing%20%28High%20Contention%29/","title":"Ticketing (High Contention)","text":""},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Ticketing%20%28High%20Contention%29/#1-problem-statement-scope","title":"1. Problem Statement &amp; Scope","text":"<p>Design a ticketing system for high-demand events (e.g., concerts, sports) that can handle massive traffic spikes, prevent overselling, and ensure fairness. The system must support temporary seat holds, payment, and confirmation, with strong consistency and resilience to failures.</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Ticketing%20%28High%20Contention%29/#2-requirements","title":"2. Requirements","text":"<ul> <li>Functional Requirements:<ul> <li>Users can view available seats in real time.</li> <li>Place temporary holds on seats (cart/hold window).</li> <li>Confirm purchase after payment.</li> <li>Release holds on timeout or user abandon.</li> <li>Waitlist/queue for overflow demand.</li> </ul> </li> <li>Non-Functional Requirements:<ul> <li>Scalability: Handle 1M+ concurrent users during peak.</li> <li>Fairness: Prevent bots, ensure first-come-first-served.</li> <li>Consistency: No overselling, strong seat allocation.</li> <li>Availability: Survive flash crowds, DDoS.</li> </ul> </li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Ticketing%20%28High%20Contention%29/#3-capacity-estimation","title":"3. Capacity Estimation","text":"<ul> <li>Peak Users: 1M concurrent.</li> <li>Seats/Event: 50k.</li> <li>Hold Window: 5 minutes.</li> <li>Purchase Rate: 10k/sec peak.</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Ticketing%20%28High%20Contention%29/#4-high-level-architecture-diagram","title":"4. High-Level Architecture Diagram","text":"<p>Code snippet</p> <p><code>graph TD     A[User] --&gt; B[CDN/WAF];     B --&gt; C[Virtual Waiting Room];     C --&gt; D[Ticket Service];     D --&gt; E[Redis (Seat Holds)];     D --&gt; F[Postgres (Durable State)];     D --&gt; G[Payment Service];     D --&gt; H[Notification Service];     D --&gt; I[Queue/Waitlist Service];</code></p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Ticketing%20%28High%20Contention%29/#5-data-schema-api-design","title":"5. Data Schema &amp; API Design","text":"<ul> <li>API:<ul> <li><code>GET /v1/events/{event_id}/seats</code>: View seat map.</li> <li><code>POST /v1/events/{event_id}/hold</code>: Place hold.</li> <li><code>POST /v1/events/{event_id}/purchase</code>: Confirm purchase.</li> </ul> </li> <li>Data Models:<ul> <li>Seats: <code>seat_id, event_id, status (available/held/sold), hold_expiry, user_id</code></li> <li>Holds (Redis): <code>hold_id, seat_ids, user_id, expires_at</code></li> <li>Orders (Postgres): <code>order_id, user_id, seat_ids, status, payment_id</code></li> </ul> </li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Ticketing%20%28High%20Contention%29/#6-detailed-component-breakdown","title":"6. Detailed Component Breakdown","text":"<ul> <li>CDN/WAF: Absorbs DDoS, caches static content.</li> <li>Virtual Waiting Room: Throttles entry, prevents overload, enforces fairness.</li> <li>Ticket Service: Core logic for seat holds, purchase, and release. Uses Redis for fast holds, Postgres for durability.</li> <li>Redis (Seat Holds): In-memory, expiring keys for temporary holds.</li> <li>Postgres (Durable State): Source of truth for sold seats and orders.</li> <li>Payment Service: Handles payment and refunds.</li> <li>Notification Service: Sends confirmation, reminders, or waitlist updates.</li> <li>Queue/Waitlist Service: Manages overflow demand.</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Ticketing%20%28High%20Contention%29/#7-end-to-end-flow-seat-hold-purchase","title":"7. End-to-End Flow (Seat Hold &amp; Purchase)","text":"<p>Code snippet</p> <p>`sequenceDiagram     participant User     participant WaitingRoom     participant TicketSvc     participant Redis     participant Postgres     participant Payment     participant Notification</p> <pre><code>User-&gt;&gt;WaitingRoom: Enter event\nWaitingRoom-&gt;&gt;TicketSvc: Allow entry\nTicketSvc-&gt;&gt;Redis: Place seat hold\nRedis--&gt;&gt;TicketSvc: Hold confirmed\nTicketSvc-&gt;&gt;User: Show hold, start timer\nUser-&gt;&gt;TicketSvc: Purchase\nTicketSvc-&gt;&gt;Payment: Process payment\nPayment--&gt;&gt;TicketSvc: Success/Fail\nalt Success\n    TicketSvc-&gt;&gt;Postgres: Mark seat(s) sold\n    TicketSvc-&gt;&gt;Redis: Remove hold\n    TicketSvc-&gt;&gt;Notification: Send confirmation\nelse Fail\n    TicketSvc-&gt;&gt;Redis: Release hold\n    TicketSvc-&gt;&gt;User: Show error\nend\n</code></pre> <p>`</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Ticketing%20%28High%20Contention%29/#8-bottlenecks-fault-tolerance-and-trade-offs","title":"8. Bottlenecks, Fault Tolerance, and Trade-offs","text":"<ul> <li>Bottleneck: Redis/DB:<ul> <li>Redis for speed, Postgres for durability. Use optimistic locking for seat updates.</li> </ul> </li> <li>Fairness:<ul> <li>Virtual waiting room and rate limiting prevent unfair access.</li> </ul> </li> <li>Consistency:<ul> <li>Optimistic locking ensures no double-sell. Redis expiry releases abandoned holds.</li> </ul> </li> <li>Trade-offs:<ul> <li>Speed vs. consistency. Strong consistency for seat allocation, eventual for notifications.</li> <li>In-memory holds are fast but require careful expiry handling.</li> </ul> </li> </ul> <p>This design is used by major ticketing platforms to handle flash sales and high contention events.</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/URL%20Shortener%20%28Core%29/","title":"URL Shortener (Core)","text":""},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/URL%20Shortener%20%28Core%29/#problem-statement","title":"Problem Statement","text":"<p>Shorten URLs, expand codes, and track clicks. Prevent malicious loops.</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/URL%20Shortener%20%28Core%29/#functional-requirements","title":"Functional Requirements","text":"<ul> <li><code>shorten(url, custom?)</code>, <code>expand(code)</code>, basic stats</li> <li>Prevent malicious loops</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/URL%20Shortener%20%28Core%29/#core-concepts","title":"Core Concepts","text":"<ul> <li><code>CodeGenerator</code> (base62 of counter or hash+collision)</li> <li><code>UrlMapping(code, longUrl, createdAt, owner)</code></li> <li><code>Store</code>, <code>StatsService</code></li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/URL%20Shortener%20%28Core%29/#high-level-design","title":"High-Level Design","text":"<ul> <li>Shortening:<ul> <li>Generate code (base62 of counter or hash)</li> <li>Store mapping in DB (code \u2192 longUrl)</li> <li>Support custom aliases (check for collision)</li> </ul> </li> <li>Expanding:<ul> <li>Lookup code in DB, return longUrl</li> <li>Track click stats (increment counter)</li> </ul> </li> <li>Malicious Loops:<ul> <li>Validate longUrl is not a shortener domain</li> </ul> </li> <li>Caching:<ul> <li>Cache code\u2192longUrl for fast redirects</li> <li>TTL for dead links</li> </ul> </li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/URL%20Shortener%20%28Core%29/#step-by-step-solution","title":"Step-by-Step Solution","text":"<ol> <li>CodeGenerator: base62 encode counter/hash</li> <li>Store: mapping and stats</li> <li>API: shorten, expand, stats endpoints</li> <li>Validation: check for loops, dead links</li> </ol>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/URL%20Shortener%20%28Core%29/#edge-cases","title":"Edge Cases","text":"<ul> <li>Custom alias collision</li> <li>Expired/deleted links</li> <li>Invalid URLs</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/URL%20Shortener%20%28Global%29/","title":"URL Shortener (Global)","text":""},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/URL%20Shortener%20%28Global%29/#1-problem-statement-scope","title":"1. Problem Statement &amp; Scope","text":"<p>Design a global URL shortening service (like bit.ly or tinyurl.com) that generates short aliases for long URLs and provides fast, highly available redirection. The system must handle billions of URLs, massive read traffic, and provide analytics.</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/URL%20Shortener%20%28Global%29/#2-requirements","title":"2. Requirements","text":"<ul> <li>Functional Requirements:<ul> <li>Users can submit a long URL and receive a unique, short URL.</li> <li>Optionally support custom aliases.</li> <li>Accessing the short URL redirects to the original long URL.</li> <li>Track click analytics (usage, geo, referrer).</li> </ul> </li> <li>Non-Functional Requirements:<ul> <li>Latency: Redirects must be extremely fast (p99 &lt; 30ms).</li> <li>Availability: Five-nines (99.999%) for redirects.</li> <li>Scalability: 50k QPS reads, billions of URLs.</li> <li>Durability: No lost mappings.</li> </ul> </li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/URL%20Shortener%20%28Global%29/#3-capacity-estimation","title":"3. Capacity Estimation","text":"<ul> <li>Write Rate: 10 new URLs/sec.</li> <li>Read Rate: 50,000 redirects/sec (read/write = 5000:1).</li> <li>Storage: 10B URLs * 600B = 6TB (sharded DB).</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/URL%20Shortener%20%28Global%29/#4-high-level-architecture-diagram","title":"4. High-Level Architecture Diagram","text":"<p>Code snippet</p> <p><code>graph TD     subgraph \"Write Path (Shorten)\"         A[User] --&gt; B[API Service];         B --&gt; C[Code Generation Service];         B --&gt; D[Database (Sharded)];     end     subgraph \"Read Path (Redirect)\"         E[User] --&gt; F[Edge CDN/PoP];         F --&gt; G[Redirector Service];         G --&gt; H[Cache (Redis)];         H --&gt; D;     end     subgraph \"Analytics Path (Async)\"         G -- fire-and-forget --&gt; I[Message Bus (Kafka)];         I --&gt; J[Stream Processor];         J --&gt; K[Analytics DB (ClickHouse)];     end</code></p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/URL%20Shortener%20%28Global%29/#5-data-schema-api-design","title":"5. Data Schema &amp; API Design","text":"<ul> <li>API:<ul> <li><code>POST /v1/shorten</code>: <code>{long_url, custom_alias?}</code> \u2192 <code>{short_url}</code></li> <li><code>GET /{short_code}</code>: Redirect endpoint.</li> </ul> </li> <li>Data Models:<ul> <li>URLs Table: <code>short_code, long_url, user_id, created_at, ...</code></li> <li>Analytics Table: <code>short_code, ts, ip, geo, referrer, ...</code></li> </ul> </li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/URL%20Shortener%20%28Global%29/#6-detailed-component-breakdown","title":"6. Detailed Component Breakdown","text":"<ul> <li>API Service: Handles URL shortening requests, validates input, and stores mappings.</li> <li>Code Generation Service: Generates unique short codes, checks for collisions, supports custom aliases.</li> <li>Database (Sharded): Stores mappings, sharded for scale.</li> <li>Edge CDN/PoP: Caches redirects close to users for low latency.</li> <li>Redirector Service: Looks up short code, issues HTTP 301/302 redirect.</li> <li>Cache (Redis): Caches hot mappings for fast lookup.</li> <li>Message Bus (Kafka): Streams click events for analytics.</li> <li>Stream Processor: Aggregates analytics, writes to analytics DB.</li> <li>Analytics DB (ClickHouse): Stores and serves analytics queries.</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/URL%20Shortener%20%28Global%29/#7-end-to-end-flow-shorten-redirect","title":"7. End-to-End Flow (Shorten &amp; Redirect)","text":"<p>Code snippet</p> <p>`sequenceDiagram     participant User     participant API     participant CodeGen     participant DB     participant CDN     participant Redirector     participant Cache     participant Kafka     participant Analytics</p> <pre><code>User-&gt;&gt;API: POST /shorten\nAPI-&gt;&gt;CodeGen: Generate code\nCodeGen-&gt;&gt;DB: Store mapping\nDB--&gt;&gt;API: Ack\nAPI--&gt;&gt;User: Return short_url\nUser-&gt;&gt;CDN: GET /{short_code}\nCDN-&gt;&gt;Redirector: Lookup\nRedirector-&gt;&gt;Cache: Check cache\nalt Hit\n    Cache--&gt;&gt;Redirector: Return long_url\nelse Miss\n    Redirector-&gt;&gt;DB: Lookup\n    DB--&gt;&gt;Redirector: Return long_url\n    Redirector-&gt;&gt;Cache: Set cache\nend\nRedirector--&gt;&gt;User: HTTP 301/302\nRedirector-&gt;&gt;Kafka: Log click\nKafka-&gt;&gt;Analytics: Aggregate\n</code></pre> <p>`</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/URL%20Shortener%20%28Global%29/#8-bottlenecks-fault-tolerance-and-trade-offs","title":"8. Bottlenecks, Fault Tolerance, and Trade-offs","text":"<ul> <li>Bottleneck: DB/Cache:<ul> <li>Use sharding and caching for scale. CDN for global low latency.</li> </ul> </li> <li>Code Generation:<ul> <li>Ensure uniqueness, avoid collisions. Use random or hash-based codes.</li> </ul> </li> <li>Analytics:<ul> <li>Async, eventual consistency. Use stream processing for scale.</li> </ul> </li> <li>Trade-offs:<ul> <li>Write path can be eventually consistent. Read path must be highly available and fast.</li> </ul> </li> </ul> <p>This design is used by Bitly, TinyURL, and other global URL shorteners.         - <code>short_code VARCHAR(8) PRIMARY KEY,</code>         - <code>long_url TEXT,</code>         - <code>created_at TIMESTAMP</code>     - The choice of <code>short_code</code> as the primary key is perfect for a sharded system, as random-looking codes will distribute the load evenly.</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/URL%20Shortener%20%28Global%29/#6-detailed-component-breakdown_1","title":"6. Detailed Component Breakdown","text":"<ul> <li>API Service (Write Path): Receives the long URL. It calls the Code Generation Service to get a unique short code, then writes the mapping (<code>short_code</code> -&gt; <code>long_url</code>) to the database. If a custom alias is requested, it first checks the DB for its availability.</li> <li>Code Generation Service: This is a critical component that must produce unique short codes without being a bottleneck.<ul> <li>Strategy 1 (Snowflake-style): Use a distributed unique ID generator (like Twitter's Snowflake) to get a 64-bit integer. Then, Base62-encode this integer <code>[a-zA-Z0-9]</code>. This produces a ~7-character code, is guaranteed to be unique, and doesn't require a database check.</li> <li>Strategy 2 (Pre-generation): Have a background job that generates millions of random, unused codes and stores them in a queue (e.g., in Redis). The API service just pops a code from this queue when needed.</li> </ul> </li> <li>Redirector Service (Read Path): A fleet of lightweight, stateless servers deployed globally. Their only job is to handle <code>GET /{short_code}</code> requests.<ol> <li>First, check a multi-layered cache (e.g., local in-memory, then a regional Redis cluster).</li> <li>If it's a cache miss, query the database to get the <code>long_url</code>.</li> <li>Populate the cache with the result.</li> <li>Return a <code>301</code> or <code>302</code> redirect.</li> <li>Asynchronously publish a click event to Kafka for analytics.</li> </ol> </li> <li>Cache: The most important part of the read path. A very high cache hit rate (&gt;99%) is expected. This shields the database from the massive read traffic. Caching can happen at multiple levels: browser, CDN edge, and the service's regional Redis cluster.</li> <li>Analytics Pipeline: A standard streaming pipeline. The fire-and-forget approach ensures that analytics processing adds zero latency to the user-facing redirect.</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/URL%20Shortener%20%28Global%29/#7-end-to-end-flow-redirect","title":"7. End-to-End Flow (Redirect)","text":"<p>Code snippet</p> <p>`sequenceDiagram     participant UserBrowser     participant EdgeCDN     participant RedirectorSvc     participant RedisCache     participant Database</p> <pre><code>UserBrowser-&gt;&gt;EdgeCDN: GET /aBcDeF\nNote over EdgeCDN: Cache MISS\nEdgeCDN-&gt;&gt;RedirectorSvc: GET /aBcDeF\n\nRedirectorSvc-&gt;&gt;RedisCache: GET short_code:aBcDeF\nNote over RedisCache: Cache MISS\nRedisCache--&gt;&gt;RedirectorSvc: nil\n\nRedirectorSvc-&gt;&gt;Database: SELECT long_url WHERE short_code='aBcDeF'\nDatabase--&gt;&gt;RedirectorSvc: \"http://very.long.url/...\"\n\nRedirectorSvc-&gt;&gt;RedisCache: SET short_code:aBcDeF \"http://...\"\n\nNote over RedirectorSvc: Asynchronously log click to Kafka\nRedirectorSvc--&gt;&gt;EdgeCDN: 302 Found, Location: \"http://...\"\nEdgeCDN--&gt;&gt;UserBrowser: 302 Found, Location: \"http://...\"`\n</code></pre>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/URL%20Shortener%20%28Global%29/#8-bottlenecks-fault-tolerance-and-trade-offs_1","title":"8. Bottlenecks, Fault Tolerance, and Trade-offs","text":"<ul> <li>Bottleneck: Write Path Code Generation: If we used a naive approach of generating a random code and checking the DB for uniqueness, we would face high latency and collisions at scale. The Snowflake/Base62 approach avoids this central contention.</li> <li>Fault Tolerance:<ul> <li>Read Path: The read path is highly resilient. If the database is temporarily unavailable, the system can continue to serve redirects for all cached entries, degrading gracefully.</li> <li>Write Path: The write path is less critical. If it's down for a few minutes, users cannot create new short URLs, but existing ones continue to work.</li> </ul> </li> <li>Key Trade-offs:<ul> <li>Redirect Type (301 vs. 302):<ul> <li><code>301 Moved Permanently</code>: The browser caches this response aggressively. The next time the user clicks the link, the browser may go directly to the long URL without ever contacting our service again. This is great for reducing server load but terrible for analytics and makes it impossible to ever change the destination URL.</li> <li><code>302 Found</code> (or <code>307 Temporary Redirect</code>): The browser is instructed that this redirect is temporary and it should always check the short URL first. This ensures our service is hit every time, allowing for 100% accurate analytics and the ability to edit the destination. The trade-off is higher traffic to our servers. For most commercial shorteners, 302/307 is the correct choice.</li> </ul> </li> </ul> </li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Vector%20Search%20Service/","title":"Vector Search Service","text":""},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Vector%20Search%20Service/#1-problem-statement-scope","title":"1. Problem Statement &amp; Scope","text":"<p>Design a scalable, low-latency vector search service for semantic search over 100M+ items using high-dimensional embeddings. The system must support fast k-NN queries, incremental updates, and metadata filtering, with high availability and horizontal scalability.</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Vector%20Search%20Service/#2-requirements","title":"2. Requirements","text":"<ul> <li>Functional Requirements:<ul> <li>Ingest and index high-dimensional vectors (e.g., 768D, 1536D).</li> <li>Support k-NN search with optional metadata filters.</li> <li>Incremental updates (add, delete, update vectors).</li> <li>Return top-k most similar items for a query vector.</li> </ul> </li> <li>Non-Functional Requirements:<ul> <li>Latency: &lt;100ms/query for 100M+ items.</li> <li>Scalability: Scale to billions of vectors via sharding.</li> <li>Availability: 99.99% uptime.</li> <li>Consistency: Eventual for index, strong for metadata.</li> </ul> </li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Vector%20Search%20Service/#3-capacity-estimation","title":"3. Capacity Estimation","text":"<ul> <li>Corpus Size: 100M vectors, 1536D, float32 = ~600MB/1M vectors, ~60GB total.</li> <li>Query Rate: 10k QPS.</li> <li>Index Size: With metadata, ~100GB total.</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Vector%20Search%20Service/#4-high-level-architecture-diagram","title":"4. High-Level Architecture Diagram","text":"<p>Code snippet</p> <p><code>graph TD     A[Client] --&gt; B[API Gateway];     B --&gt; C[Embedding Model Service];     B --&gt; D[Query Router];     D --&gt; E[Shard 1 (Faiss/HNSW)];     D --&gt; F[Shard 2 (Faiss/HNSW)];     D --&gt; G[Shard N (Faiss/HNSW)];     E --&gt; H[Metadata Store];     F --&gt; H;     G --&gt; H;     D --&gt; I[Aggregator/Ranker];     I --&gt; J[Result];</code></p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Vector%20Search%20Service/#5-data-schema-api-design","title":"5. Data Schema &amp; API Design","text":"<ul> <li>API:<ul> <li><code>POST /v1/search</code>: <code>{query_vector, k, filters}</code></li> <li><code>POST /v1/index</code>: <code>{item_id, vector, metadata}</code></li> <li><code>DELETE /v1/index/{item_id}</code></li> </ul> </li> <li>Data Models:<ul> <li>Vector Index: HNSW/IVF-PQ, sharded by item_id hash.</li> <li>Metadata Store: <code>item_id, metadata fields...</code></li> </ul> </li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Vector%20Search%20Service/#6-detailed-component-breakdown","title":"6. Detailed Component Breakdown","text":"<ul> <li>Embedding Model Service: Converts raw data (text, image) to vectors.</li> <li>API Gateway: Handles authentication, rate limiting, and forwards requests.</li> <li>Query Router: Routes search requests to relevant shards, aggregates results.</li> <li>Shard (Faiss/HNSW): In-memory or SSD-based vector index for fast k-NN search.</li> <li>Metadata Store: Stores item metadata for filtering and re-ranking.</li> <li>Aggregator/Ranker: Merges results from shards, applies filters, sorts by similarity.</li> </ul>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Vector%20Search%20Service/#7-end-to-end-flow-search-query","title":"7. End-to-End Flow (Search Query)","text":"<p>Code snippet</p> <p>`sequenceDiagram     participant Client     participant APIGW     participant Embed     participant Router     participant Shard1     participant Shard2     participant Agg</p> <pre><code>Client-&gt;&gt;APIGW: POST /v1/search\nAPIGW-&gt;&gt;Embed: Get query vector\nEmbed--&gt;&gt;APIGW: Return vector\nAPIGW-&gt;&gt;Router: Forward search\nRouter-&gt;&gt;Shard1: k-NN search\nRouter-&gt;&gt;Shard2: k-NN search\nShard1--&gt;&gt;Router: Top-k results\nShard2--&gt;&gt;Router: Top-k results\nRouter-&gt;&gt;Agg: Aggregate, filter, rank\nAgg--&gt;&gt;APIGW: Final results\nAPIGW--&gt;&gt;Client: Return top-k\n</code></pre> <p>`</p>"},{"location":"hld/DE%20Shaw%20HLD%20with%20Implementations/Vector%20Search%20Service/#8-bottlenecks-fault-tolerance-and-trade-offs","title":"8. Bottlenecks, Fault Tolerance, and Trade-offs","text":"<ul> <li>Bottleneck: Shard Memory/CPU:<ul> <li>Use SSD-based indexes for larger-than-memory datasets. Horizontal sharding for scale.</li> </ul> </li> <li>Recall vs. Latency:<ul> <li>HNSW/IVF-PQ trade off accuracy for speed. Tune parameters per use case.</li> </ul> </li> <li>Fault Tolerance:<ul> <li>Replicate shards, use stateless routers. Failed shard = partial results, degrade gracefully.</li> </ul> </li> <li>Trade-offs:<ul> <li>Eventual consistency for index updates. Strong consistency for metadata.</li> <li>SSD-based search is slower but cheaper than RAM.</li> </ul> </li> </ul> <p>This design is used by modern semantic search engines (e.g., Pinecone, Weaviate, OpenAI) for large-scale vector search.</p>"},{"location":"lld/module2/","title":"Module 2 \u2013 Notification Service (LLD)","text":""},{"location":"lld/module2/#overview","title":"Overview","text":"<p>The Notification Service handles all outgoing notifications\u2014such as emails, SMS messages, and push notifications\u2014triggered by various system events.</p>"},{"location":"lld/module2/#class-diagram","title":"Class Diagram","text":""},{"location":"lld/module2/#main-classes-and-their-roles","title":"Main Classes and Their Roles","text":""},{"location":"lld/module2/#1-notificationmanager","title":"1. NotificationManager","text":"<p>Central coordinator for all notification events. Determines channel(s) and dispatches to the appropriate sender class.</p> <pre><code>class NotificationManager:\n    def __init__(self, email_sender, sms_sender, push_notifier):\n        self.email_sender = email_sender\n        self.sms_sender = sms_sender\n        self.push_notifier = push_notifier\n\n    def send_notification(self, user, message, channels):\n        \"\"\"Send notification to user via specified channels.\"\"\"\n        results = {}\n        if \"email\" in channels:\n            results[\"email\"] = self.email_sender.send_email(user.email, message)\n        if \"sms\" in channels:\n            results[\"sms\"] = self.sms_sender.send_sms(user.phone, message)\n        if \"push\" in channels:\n            results[\"push\"] = self.push_notifier.send_push(user.device_id, message)\n        return results\n</code></pre>"},{"location":"lld/module2/#2-emailsender","title":"2. EmailSender","text":"<p>Handles all email-related sending logic (using SMTP or an external service).</p> <pre><code>import smtplib\n\nclass EmailSender:\n    def __init__(self, smtp_server, port, login, password):\n        self.smtp_server = smtp_server\n        self.port = port\n        self.login = login\n        self.password = password\n\n    def send_email(self, recipient, message):\n        # Placeholder; use real library in prod!\n        print(f\"Sending EMAIL to {recipient}: {message}\")\n        return True\n</code></pre>"},{"location":"lld/module2/#3-smssender","title":"3. SmsSender","text":"<p>Handles SMS sending (using a third-party gateway API).</p> <pre><code>class SmsSender:\n    def __init__(self, api_key):\n        self.api_key = api_key\n\n    def send_sms(self, phone, message):\n        print(f\"Sending SMS to {phone}: {message}\")\n        return True\n</code></pre>"},{"location":"lld/module2/#4-pushnotifier","title":"4. PushNotifier","text":"<p>Handles push notifications (e.g., using Firebase Cloud Messaging).</p> <pre><code>class PushNotifier:\n    def __init__(self, fcm_key):\n        self.fcm_key = fcm_key\n\n    def send_push(self, device_id, message):\n        print(f\"Sending PUSH notification to device {device_id}: {message}\")\n        return True\n</code></pre>"},{"location":"lld/module2/#example-usage","title":"Example Usage","text":"<pre><code># Example user object\nclass User:\n    def __init__(self, email, phone, device_id):\n        self.email = email\n        self.phone = phone\n        self.device_id = device_id\n\nemail_sender = EmailSender(\"smtp.mail.com\", 587, \"login\", \"password\")\nsms_sender = SmsSender(\"api-key-123\")\npush_notifier = PushNotifier(\"fcm-key-xyz\")\n\nnotifier = NotificationManager(email_sender, sms_sender, push_notifier)\n\nuser = User(\"test@example.com\", \"+911234567890\", \"device123\")\nchannels = [\"email\", \"push\"]\nnotifier.send_notification(user, \"Welcome to System Design Hub!\", channels)\n</code></pre>"},{"location":"lld/module2/#sequence-flow","title":"Sequence Flow","text":"<ol> <li>An event triggers a notification.</li> <li><code>NotificationManager.send_notification()</code> is called.</li> <li>It decides channels, then delegates sending to each sender class.</li> <li>Each sender class implements the sending logic (log, API call, etc).</li> </ol>"},{"location":"lld/module2/#error-handling-and-extensibility","title":"Error Handling and Extensibility","text":"<ul> <li>Each sender can log errors, retry failed deliveries, or raise exceptions.</li> <li>New channels (e.g., WhatsAppSender) can be added by extending the manager.</li> </ul> <p>Expand this code with your real-world integrations, error handling, and configs!</p>"}]}